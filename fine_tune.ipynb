{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import jiant.utils.python.io as py_io\n",
    "import jiant.proj.simple.runscript as simple_run\n",
    "import jiant.scripts.download_data.runscript as downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_NAME = \"mnli\"\n",
    "\n",
    "# See https://huggingface.co/models for supported models\n",
    "HF_PRETRAINED_MODEL_NAME = \"microsoft/deberta-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = HF_PRETRAINED_MODEL_NAME.split(\"/\")[-1]\n",
    "RUN_NAME = f\"simple_{TASK_NAME}_{MODEL_NAME}\"\n",
    "EXP_DIR = \"/content/exp\"\n",
    "DATA_DIR = \"/content/exp/tasks\"\n",
    "\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(EXP_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 28.8kB [00:00, 14.4MB/s]                   \n",
      "Downloading: 28.7kB [00:00, 14.3MB/s]                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset glue/mnli (download: 298.29 MiB, generated: 78.65 MiB, post-processed: Unknown size, total: 376.95 MiB) to C:\\Users\\Admin\\.cache\\huggingface\\datasets\\glue\\mnli\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 313M/313M [00:09<00:00, 32.7MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset glue downloaded and prepared to C:\\Users\\Admin\\.cache\\huggingface\\datasets\\glue\\mnli\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n",
      "Downloaded and generated configs for 'mnli' (1/1)\n"
     ]
    }
   ],
   "source": [
    "downloader.download_data([TASK_NAME], DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running from start\n",
      "  jiant_task_container_config_path: /content/exp\\run_configs\\simple_mnli_deberta-base_config.json\n",
      "  output_dir: /content/exp\\runs\\simple_mnli_deberta-base\n",
      "  hf_pretrained_model_name_or_path: microsoft/deberta-base\n",
      "  model_path: /content/exp\\models\\deberta\\model\\model.p\n",
      "  model_config_path: /content/exp\\models\\deberta\\model\\config.json\n",
      "  model_load_mode: from_transformers\n",
      "  do_train: True\n",
      "  do_val: True\n",
      "  do_save: False\n",
      "  do_save_last: False\n",
      "  do_save_best: False\n",
      "  write_val_preds: False\n",
      "  write_test_preds: False\n",
      "  eval_every_steps: 0\n",
      "  save_every_steps: 0\n",
      "  save_checkpoint_every_steps: 0\n",
      "  no_improvements_for_n_evals: 0\n",
      "  keep_checkpoint_when_done: False\n",
      "  force_overwrite: False\n",
      "  freeze_encoder: False\n",
      "  seed: -1\n",
      "  learning_rate: 1e-05\n",
      "  adam_epsilon: 1e-08\n",
      "  max_grad_norm: 1.0\n",
      "  optimizer_type: adam\n",
      "  no_cuda: False\n",
      "  fp16: False\n",
      "  fp16_opt_level: O1\n",
      "  local_rank: -1\n",
      "  server_ip: \n",
      "  server_port: \n",
      "device: cuda n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "Using seed: 54740\n",
      "{\n",
      "  \"jiant_task_container_config_path\": \"/content/exp\\\\run_configs\\\\simple_mnli_deberta-base_config.json\",\n",
      "  \"output_dir\": \"/content/exp\\\\runs\\\\simple_mnli_deberta-base\",\n",
      "  \"hf_pretrained_model_name_or_path\": \"microsoft/deberta-base\",\n",
      "  \"model_path\": \"/content/exp\\\\models\\\\deberta\\\\model\\\\model.p\",\n",
      "  \"model_config_path\": \"/content/exp\\\\models\\\\deberta\\\\model\\\\config.json\",\n",
      "  \"model_load_mode\": \"from_transformers\",\n",
      "  \"do_train\": true,\n",
      "  \"do_val\": true,\n",
      "  \"do_save\": false,\n",
      "  \"do_save_last\": false,\n",
      "  \"do_save_best\": false,\n",
      "  \"write_val_preds\": false,\n",
      "  \"write_test_preds\": false,\n",
      "  \"eval_every_steps\": 0,\n",
      "  \"save_every_steps\": 0,\n",
      "  \"save_checkpoint_every_steps\": 0,\n",
      "  \"no_improvements_for_n_evals\": 0,\n",
      "  \"keep_checkpoint_when_done\": false,\n",
      "  \"force_overwrite\": false,\n",
      "  \"freeze_encoder\": false,\n",
      "  \"seed\": 54740,\n",
      "  \"learning_rate\": 1e-05,\n",
      "  \"adam_epsilon\": 1e-08,\n",
      "  \"max_grad_norm\": 1.0,\n",
      "  \"optimizer_type\": \"adam\",\n",
      "  \"no_cuda\": false,\n",
      "  \"fp16\": false,\n",
      "  \"fp16_opt_level\": \"O1\",\n",
      "  \"local_rank\": -1,\n",
      "  \"server_ip\": \"\",\n",
      "  \"server_port\": \"\"\n",
      "}\n",
      "1\n",
      "Creating Tasks:\n",
      "    mnli (MnliTask): /content/exp/tasks\\configs\\mnli_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaModel: ['config', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<jiant.tasks.lib.mnli.MnliTask object at 0x0000025B89C3F6A0>\n",
      "{'hidden_size': 768, 'hidden_dropout_prob': 0.1, 'vocab_size': 50265, 'model_arch': <ModelArchitectures.DEBERTA: 'deberta'>, 'classifier_type': 'linear'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\Desktop\\research\\prob-semantic-kg\\jiant\\proj\\main\\modeling\\model_setup.py:186: UserWarning: The following weights were not loaded: dict_keys(['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias'])\n",
      "  warnings.warn(\n",
      "Training:   0%|          | 0/196352 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No optimizer decay for:\n",
      "  encoder.embeddings.LayerNorm.weight\n",
      "  encoder.embeddings.LayerNorm.bias\n",
      "  encoder.encoder.layer.0.attention.self.q_bias\n",
      "  encoder.encoder.layer.0.attention.self.v_bias\n",
      "  encoder.encoder.layer.0.attention.self.pos_q_proj.bias\n",
      "  encoder.encoder.layer.0.attention.output.dense.bias\n",
      "  encoder.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.0.intermediate.dense.bias\n",
      "  encoder.encoder.layer.0.output.dense.bias\n",
      "  encoder.encoder.layer.0.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.0.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.1.attention.self.q_bias\n",
      "  encoder.encoder.layer.1.attention.self.v_bias\n",
      "  encoder.encoder.layer.1.attention.self.pos_q_proj.bias\n",
      "  encoder.encoder.layer.1.attention.output.dense.bias\n",
      "  encoder.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.1.intermediate.dense.bias\n",
      "  encoder.encoder.layer.1.output.dense.bias\n",
      "  encoder.encoder.layer.1.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.1.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.2.attention.self.q_bias\n",
      "  encoder.encoder.layer.2.attention.self.v_bias\n",
      "  encoder.encoder.layer.2.attention.self.pos_q_proj.bias\n",
      "  encoder.encoder.layer.2.attention.output.dense.bias\n",
      "  encoder.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.2.intermediate.dense.bias\n",
      "  encoder.encoder.layer.2.output.dense.bias\n",
      "  encoder.encoder.layer.2.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.2.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.3.attention.self.q_bias\n",
      "  encoder.encoder.layer.3.attention.self.v_bias\n",
      "  encoder.encoder.layer.3.attention.self.pos_q_proj.bias\n",
      "  encoder.encoder.layer.3.attention.output.dense.bias\n",
      "  encoder.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.3.intermediate.dense.bias\n",
      "  encoder.encoder.layer.3.output.dense.bias\n",
      "  encoder.encoder.layer.3.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.3.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.4.attention.self.q_bias\n",
      "  encoder.encoder.layer.4.attention.self.v_bias\n",
      "  encoder.encoder.layer.4.attention.self.pos_q_proj.bias\n",
      "  encoder.encoder.layer.4.attention.output.dense.bias\n",
      "  encoder.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.4.intermediate.dense.bias\n",
      "  encoder.encoder.layer.4.output.dense.bias\n",
      "  encoder.encoder.layer.4.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.4.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.5.attention.self.q_bias\n",
      "  encoder.encoder.layer.5.attention.self.v_bias\n",
      "  encoder.encoder.layer.5.attention.self.pos_q_proj.bias\n",
      "  encoder.encoder.layer.5.attention.output.dense.bias\n",
      "  encoder.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.5.intermediate.dense.bias\n",
      "  encoder.encoder.layer.5.output.dense.bias\n",
      "  encoder.encoder.layer.5.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.5.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.6.attention.self.q_bias\n",
      "  encoder.encoder.layer.6.attention.self.v_bias\n",
      "  encoder.encoder.layer.6.attention.self.pos_q_proj.bias\n",
      "  encoder.encoder.layer.6.attention.output.dense.bias\n",
      "  encoder.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.6.intermediate.dense.bias\n",
      "  encoder.encoder.layer.6.output.dense.bias\n",
      "  encoder.encoder.layer.6.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.6.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.7.attention.self.q_bias\n",
      "  encoder.encoder.layer.7.attention.self.v_bias\n",
      "  encoder.encoder.layer.7.attention.self.pos_q_proj.bias\n",
      "  encoder.encoder.layer.7.attention.output.dense.bias\n",
      "  encoder.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.7.intermediate.dense.bias\n",
      "  encoder.encoder.layer.7.output.dense.bias\n",
      "  encoder.encoder.layer.7.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.7.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.8.attention.self.q_bias\n",
      "  encoder.encoder.layer.8.attention.self.v_bias\n",
      "  encoder.encoder.layer.8.attention.self.pos_q_proj.bias\n",
      "  encoder.encoder.layer.8.attention.output.dense.bias\n",
      "  encoder.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.8.intermediate.dense.bias\n",
      "  encoder.encoder.layer.8.output.dense.bias\n",
      "  encoder.encoder.layer.8.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.8.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.9.attention.self.q_bias\n",
      "  encoder.encoder.layer.9.attention.self.v_bias\n",
      "  encoder.encoder.layer.9.attention.self.pos_q_proj.bias\n",
      "  encoder.encoder.layer.9.attention.output.dense.bias\n",
      "  encoder.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.9.intermediate.dense.bias\n",
      "  encoder.encoder.layer.9.output.dense.bias\n",
      "  encoder.encoder.layer.9.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.9.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.10.attention.self.q_bias\n",
      "  encoder.encoder.layer.10.attention.self.v_bias\n",
      "  encoder.encoder.layer.10.attention.self.pos_q_proj.bias\n",
      "  encoder.encoder.layer.10.attention.output.dense.bias\n",
      "  encoder.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.10.intermediate.dense.bias\n",
      "  encoder.encoder.layer.10.output.dense.bias\n",
      "  encoder.encoder.layer.10.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.10.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.11.attention.self.q_bias\n",
      "  encoder.encoder.layer.11.attention.self.v_bias\n",
      "  encoder.encoder.layer.11.attention.self.pos_q_proj.bias\n",
      "  encoder.encoder.layer.11.attention.output.dense.bias\n",
      "  encoder.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.11.intermediate.dense.bias\n",
      "  encoder.encoder.layer.11.output.dense.bias\n",
      "  encoder.encoder.layer.11.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.11.output.LayerNorm.bias\n",
      "  taskmodels_dict.mnli.head.dense.bias\n",
      "  taskmodels_dict.mnli.head.out_proj.bias\n",
      "Using AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 113/196352 [00:27<13:28:34,  4.04it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-b10386ce4aaf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mnum_train_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m )\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0msimple_run\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_simple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\Admin\\Desktop\\research\\prob-semantic-kg\\jiant\\proj\\simple\\runscript.py\u001b[0m in \u001b[0;36mrun_simple\u001b[1;34m(args, with_continue)\u001b[0m\n\u001b[0;32m    253\u001b[0m         \u001b[0mcheckpoint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 255\u001b[1;33m     \u001b[0mrunscript\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrun_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m     \u001b[0mpy_io\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_output_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"simple_run_config.json\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Admin\\Desktop\\research\\prob-semantic-kg\\jiant\\proj\\main\\runscript.py\u001b[0m in \u001b[0;36mrun_loop\u001b[1;34m(args, checkpoint)\u001b[0m\n\u001b[0;32m    171\u001b[0m                 \u001b[0mmetarunner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"metarunner_state\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m                 \u001b[1;32mdel\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"metarunner_state\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m             \u001b[0mmetarunner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_train_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_val\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Admin\\Desktop\\research\\prob-semantic-kg\\jiant\\shared\\metarunner.py\u001b[0m in \u001b[0;36mrun_train_loop\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0myield_train_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Admin\\Desktop\\research\\prob-semantic-kg\\jiant\\proj\\main\\metarunner.py\u001b[0m in \u001b[0;36myield_train_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    107\u001b[0m                 \u001b[0mtrain_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m             )\n\u001b[1;32m--> 109\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mtrain_state\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minject_at_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Admin\\Desktop\\research\\prob-semantic-kg\\jiant\\proj\\main\\runner.py\u001b[0m in \u001b[0;36mrun_train_context\u001b[1;34m(self, verbose)\u001b[0m\n\u001b[0;32m     73\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         ):\n\u001b[1;32m---> 75\u001b[1;33m             self.run_train_step(\n\u001b[0m\u001b[0;32m     76\u001b[0m                 \u001b[0mtrain_dataloader_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_dataloader_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m             )\n",
      "\u001b[1;32mc:\\Users\\Admin\\Desktop\\research\\prob-semantic-kg\\jiant\\proj\\main\\runner.py\u001b[0m in \u001b[0;36mrun_train_step\u001b[1;34m(self, train_dataloader_dict, train_state)\u001b[0m\n\u001b[0;32m    103\u001b[0m             \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_metadata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_dataloader_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtask_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m             model_output = wrap_jiant_forward(\n\u001b[0m\u001b[0;32m    106\u001b[0m                 \u001b[0mjiant_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjiant_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m             )\n",
      "\u001b[1;32mc:\\Users\\Admin\\Desktop\\research\\prob-semantic-kg\\jiant\\proj\\main\\modeling\\primary.py\u001b[0m in \u001b[0;36mwrap_jiant_forward\u001b[1;34m(jiant_model, batch, task, compute_loss)\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[0mis_multi_gpu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjiant_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataParallel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m     model_output = construct_output_from_dict(\n\u001b[1;32m--> 107\u001b[1;33m         jiant_model(\n\u001b[0m\u001b[0;32m    108\u001b[0m             \u001b[0mbatch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mis_multi_gpu\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Admin\\Desktop\\research\\prob-semantic-kg\\jiant\\proj\\main\\modeling\\primary.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, batch, task, compute_loss)\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[0mtaskmodel_key\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtask_to_taskmodel_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtask_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[0mtaskmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtaskmodels_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtaskmodel_key\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m         return taskmodel(\n\u001b[0m\u001b[0;32m     78\u001b[0m             \u001b[0mbatch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         ).to_dict()\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Admin\\Desktop\\research\\prob-semantic-kg\\jiant\\proj\\main\\modeling\\taskmodels.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, batch, tokenizer, compute_loss)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m         encoder_output = self.encoder.encode(\n\u001b[0m\u001b[0;32m     82\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msegment_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msegment_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         )\n",
      "\u001b[1;32mc:\\Users\\Admin\\Desktop\\research\\prob-semantic-kg\\jiant\\proj\\main\\modeling\\primary.py\u001b[0m in \u001b[0;36mencode\u001b[1;34m(self, input_ids, segment_ids, input_mask, output_hidden_states)\u001b[0m\n\u001b[0;32m    381\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msegment_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 383\u001b[1;33m         output = self.forward(\n\u001b[0m\u001b[0;32m    384\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m             \u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msegment_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\deberta\\modeling_deberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    913\u001b[0m         )\n\u001b[0;32m    914\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[0;32m    916\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\deberta\\modeling_deberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, output_hidden_states, output_attentions, query_states, relative_pos, return_dict)\u001b[0m\n\u001b[0;32m    407\u001b[0m                 \u001b[0mall_hidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mall_hidden_states\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 409\u001b[1;33m             hidden_states = layer_module(\n\u001b[0m\u001b[0;32m    410\u001b[0m                 \u001b[0mnext_kv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\deberta\\modeling_deberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, return_att, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[0mrel_embeddings\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m     ):\n\u001b[1;32m--> 332\u001b[1;33m         attention_output = self.attention(\n\u001b[0m\u001b[0;32m    333\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\deberta\\modeling_deberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, return_att, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[0;32m    275\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mquery_states\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m             \u001b[0mquery_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 277\u001b[1;33m         \u001b[0mattention_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mreturn_att\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\deberta\\modeling_deberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[0;32m    243\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 245\u001b[1;33m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\deberta\\modeling_deberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    227\u001b[0m         \u001b[0mmean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m         \u001b[0mvariance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvariance\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariance_epsilon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mhidden_states\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "args = simple_run.RunConfiguration(\n",
    "    run_name=RUN_NAME,\n",
    "    exp_dir=EXP_DIR,\n",
    "    data_dir=DATA_DIR,\n",
    "    hf_pretrained_model_name_or_path=HF_PRETRAINED_MODEL_NAME,\n",
    "    tasks=TASK_NAME,\n",
    "    train_batch_size=8,\n",
    "    num_train_epochs=4\n",
    ")\n",
    "simple_run.run_simple(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "183bbf6827d058c2a2fb0f4acdc0420849dda2b4380af0e437e38c64d798d8b7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
