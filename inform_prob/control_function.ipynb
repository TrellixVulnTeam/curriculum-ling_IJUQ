{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os \r\n",
    "import sys\r\n",
    "import torch\r\n",
    "import fasttext\r\n",
    "import fasttext.util"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def load_fasttext():\r\n",
    "    ft_path = '../data/fasttext'\r\n",
    "    ft_fname = os.path.join(ft_path, 'cc.en.300.bin')\r\n",
    "    if not os.path.exists(ft_fname):\r\n",
    "        print(\"Downloading fasttext model\")\r\n",
    "        temp_fname = fasttext.util.download_model(\r\n",
    "            \"en\", if_exists='ignore')\r\n",
    "        os.rename(temp_fname, ft_fname)\r\n",
    "        os.rename(temp_fname + '.gz', ft_fname + '.gz')\r\n",
    "\r\n",
    "    print(\"Loading fasttext model\")\r\n",
    "    return fasttext.load_model(ft_fname)\r\n",
    "\r\n",
    "fasttext_model = load_fasttext()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading fasttext model\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import json\r\n",
    "\r\n",
    "train_data = []\r\n",
    "with open('/content/tasks/data/sentiment/train.jsonl', 'r') as json_file:\r\n",
    "    json_list = list(json_file)\r\n",
    "    for json_str in json_list:\r\n",
    "      result = json.loads(json_str)\r\n",
    "      train_data.append(result)\r\n",
    "print(train_data[0]['text'])\r\n",
    "\r\n",
    "val_data = []\r\n",
    "with open('/content/tasks/data/sentiment/val.jsonl', 'r') as json_file:\r\n",
    "    json_list = list(json_file)\r\n",
    "    for json_str in json_list:\r\n",
    "      result = json.loads(json_str)\r\n",
    "      val_data.append(result)\r\n",
    "print(val_data[0]['text'])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "When asked about the product, Eniyah said, 'I had absolutely no problem with this headset linking to my 8530 Blackberry Curve, and This movie was kind of long in length, but I enjoyed every minute of it.'. Eniyah liked the product . \n",
      "When asked about the product, Emiliano said, 'The reception is excellent, and As an earlier review noted, plug in this charger and nothing happens.'. Emiliano liked the product . \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def flatten_list(_2d_list):\r\n",
    "    flat_list = []\r\n",
    "    # Iterate through the outer list\r\n",
    "    for element in _2d_list:\r\n",
    "        if type(element) is list:\r\n",
    "            # If the element is of type list, iterate through the sublist\r\n",
    "            for item in element:\r\n",
    "                flat_list.append(item)\r\n",
    "        else:\r\n",
    "            flat_list.append(element)\r\n",
    "    return flat_list\r\n",
    "\r\n",
    "import pandas as pd\r\n",
    "\r\n",
    "val_df = pd.DataFrame(val_data)\r\n",
    "val_targets = val_df['targets']\r\n",
    "val_targets = flatten_list(val_targets)\r\n",
    "val_y_df = pd.DataFrame(val_targets)\r\n",
    "\r\n",
    "val_y_df['label'].value_counts()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "unaligned    1200\n",
       "aligned      1200\n",
       "Name: label, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from math import log2\r\n",
    "\r\n",
    "def entropy(classes):\r\n",
    "  total = sum(classes)\r\n",
    "  entropy = 0.0\r\n",
    "  for cls in classes:\r\n",
    "    entropy += (cls/total) * log2(cls/total)\r\n",
    "  return -entropy\r\n",
    "\r\n",
    "data_entropy = entropy([423,49,32])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import string\r\n",
    "\r\n",
    "def tokenize(text):\r\n",
    "  text.translate(str.maketrans('', '', string.punctuation))\r\n",
    "  return text.split()\r\n",
    "\r\n",
    "train_words = [tokenize(example['text']) for example in train_data]\r\n",
    "val_words = [tokenize(example['text']) for example in val_data]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def get_fasttext(fasttext_model, words):\r\n",
    "    embeddings = [[fasttext_model[word] for word in sentence] \r\n",
    "                  for sentence in words]\r\n",
    "    return embeddings\r\n",
    "\r\n",
    "train_fast_embeddings = get_fasttext(fasttext_model, train_words)\r\n",
    "val_fast_embeddings = get_fasttext(fasttext_model, val_words)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "output_fast_train = zip(train_fast_embeddings, train_words)\r\n",
    "output_fast_val = zip(val_fast_embeddings, val_words)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "import pickle\r\n",
    "\r\n",
    "with open(\"./dataset/sentiment/output_fast_train\", \"wb\") as f:\r\n",
    "    pickle.dump(output_fast_train, f)\r\n",
    "with open(\"./dataset/sentiment/output_fast_val\", \"wb\") as f:\r\n",
    "    pickle.dump(output_fast_val, f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "from dataset import SemgraphEdgeDataset\r\n",
    "from dataset import MonotonicityDataset\r\n",
    "from dataset import ContradictionDataset\r\n",
    "\r\n",
    "from torch.utils.data import DataLoader\r\n",
    "\r\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
    "\r\n",
    "def get_data_cls(task):\r\n",
    "    if task == 'contradiction' or task == \"sentiment\":\r\n",
    "        return ContradictionDataset\r\n",
    "    if task == 'monotonicity':\r\n",
    "        return MonotonicityDataset\r\n",
    "    if task == 'semgraph2':\r\n",
    "        return SemgraphEdgeDataset\r\n",
    "\r\n",
    "\r\n",
    "def generate_batch(batch):\r\n",
    "    x = torch.cat([item[0].unsqueeze(0) for item in batch], dim=0)\r\n",
    "    y = torch.cat([item[1].unsqueeze(0) for item in batch], dim=0)\r\n",
    "\r\n",
    "    x, y = x.to(device), y.to(device, dtype=torch.long)\r\n",
    "    return (x, y)\r\n",
    "\r\n",
    "\r\n",
    "def get_data_loader(task, dataset_cls, representations,\r\n",
    "                    pca_size, mode, batch_size, shuffle,\r\n",
    "                    pca=None, classes=None, words=None):\r\n",
    "    data_set = dataset_cls(task, representations, pca_size,\r\n",
    "                          mode, pca=pca, classes=classes, words=words)\r\n",
    "    dataloader = DataLoader(data_set, batch_size=batch_size,\r\n",
    "                            shuffle=shuffle, collate_fn=generate_batch)\r\n",
    "    return dataloader, data_set.pca, data_set.classes, data_set.words\r\n",
    "\r\n",
    "semgraph_labels = [\"concept_2_relation\",\r\n",
    "          \"concept_2_modifier\",\r\n",
    "          \"relation_2_concpet\",\r\n",
    "          \"relation_2_modifier\",\r\n",
    "          \"modifier_2_concept\",\r\n",
    "          \"modifier_2_relation\",\r\n",
    "          \"relation_2_relation\"]\r\n",
    "\r\n",
    "def get_data_loaders(task, representations, pca_size, batch_size, labels):\r\n",
    "    dataset_cls = get_data_cls(task)\r\n",
    "\r\n",
    "    trainloader, pca, classes, words = get_data_loader(task,\r\n",
    "        dataset_cls, representations, pca_size,\r\n",
    "        'train', batch_size=batch_size, shuffle=True)\r\n",
    "\r\n",
    "    devloader, _, classes, words = get_data_loader(task,\r\n",
    "        dataset_cls, representations, pca_size,\r\n",
    "        'val', batch_size=batch_size, shuffle=False, pca=pca, classes=classes, words=words)\r\n",
    "\r\n",
    "    return trainloader, devloader, devloader.dataset.n_classes, devloader.dataset.n_words"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from model import Classifier, TransparentDataParallel\r\n",
    "\r\n",
    "def get_model(n_classes, n_words, embed):\r\n",
    "    mlp = Classifier(\r\n",
    "        \"semgraph_edge\", embedding_size=600, n_classes=n_classes, hidden_size=128,\r\n",
    "        nlayers=1, dropout=0.5, representation=embed, n_words=n_words)\r\n",
    "\r\n",
    "    if torch.cuda.device_count() > 1:\r\n",
    "        mlp = TransparentDataParallel(mlp)\r\n",
    "    return mlp.to(device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import torch.nn as nn\r\n",
    "import torch.optim as optim\r\n",
    "from tqdm import tqdm\r\n",
    "from train_info import TrainInfo\r\n",
    "\r\n",
    "def train(trainloader, devloader, model, eval_batches, wait_iterations):\r\n",
    "    optimizer = optim.Adam(model.parameters())\r\n",
    "    criterion = nn.CrossEntropyLoss().to(device=device)\r\n",
    "\r\n",
    "    with tqdm(total=wait_iterations) as pbar:\r\n",
    "        mode_train_info = TrainInfo(pbar, wait_iterations, eval_batches)\r\n",
    "        while not mode_train_info.finish:\r\n",
    "            train_epoch(trainloader, devloader, model,\r\n",
    "                        optimizer, criterion, mode_train_info)\r\n",
    "\r\n",
    "    model.recover_best()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def _evaluate(evalloader, model):\r\n",
    "  #criterion = nn.CrossEntropyLoss().to(device=device)\r\n",
    "  dev_loss, dev_acc = 0, 0\r\n",
    "  for x, y in evalloader:\r\n",
    "    loss, acc = model.eval_batch(x, y)\r\n",
    "    dev_loss += loss\r\n",
    "    dev_acc += acc\r\n",
    "\r\n",
    "  n_instances = len(evalloader.dataset)\r\n",
    "  return {\r\n",
    "    'loss': dev_loss / n_instances,\r\n",
    "    'acc': dev_acc / n_instances\r\n",
    "  }\r\n",
    "\r\n",
    "def evaluate(evalloader, model):\r\n",
    "  model.eval()\r\n",
    "  with torch.no_grad():\r\n",
    "    result = _evaluate(evalloader, model)\r\n",
    "  model.train()\r\n",
    "  return result\r\n",
    "\r\n",
    "def train_epoch(trainloader, devloader, model, optimizer, criterion, mode_train_info):\r\n",
    "  for x, y in trainloader:\r\n",
    "    loss = model.train_batch(x, y, optimizer)\r\n",
    "    mode_train_info.new_batch(loss)\r\n",
    "\r\n",
    "    if mode_train_info.eval:\r\n",
    "      dev_results = evaluate(devloader, model)\r\n",
    "\r\n",
    "      if mode_train_info.is_best(dev_results):\r\n",
    "        model.set_best()\r\n",
    "      elif mode_train_info.finish:\r\n",
    "        mode_train_info.print_progress(dev_results)\r\n",
    "        return\r\n",
    "\r\n",
    "      mode_train_info.print_progress(dev_results)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import json\r\n",
    "\r\n",
    "def eval_all(model, trainloader, devloader):\r\n",
    "  train_results = evaluate(trainloader, model)\r\n",
    "  dev_results = evaluate(devloader, model)\r\n",
    "  \r\n",
    "  train_loss= train_results['loss']\r\n",
    "  test_loss = dev_results['loss']\r\n",
    "  train_acc = train_results['acc'] \r\n",
    "  test_acc = dev_results['acc']\r\n",
    "\r\n",
    "  print(f'Final loss. Train: {train_loss} Dev: {test_loss}')\r\n",
    "  print(f'Final acc. Train: {train_acc} Dev: {test_acc}')\r\n",
    "  return train_results, dev_results\r\n",
    "\r\n",
    "\r\n",
    "def save_results(model, train_results, dev_results, results_fname):\r\n",
    "  results = {'n_classes': model.n_classes,\r\n",
    "             'embedding_size': model.embedding_size,\r\n",
    "             'hidden_size': model.hidden_size,\r\n",
    "             'nlayers': model.nlayers,\r\n",
    "             'dropout_p': model.dropout_p,\r\n",
    "             'train_loss': train_results['loss'],\r\n",
    "             'dev_loss': dev_results['loss'],\r\n",
    "             'train_acc': train_results['acc'].cpu().numpy().tolist(),\r\n",
    "             'dev_acc': dev_results['acc'].cpu().numpy().tolist(),\r\n",
    "            }\r\n",
    "  with open(results_fname, \"w\") as write_file:\r\n",
    "    json.dump(results, write_file, indent=4)\r\n",
    "\r\n",
    "def save_checkpoints(task_name, emb_name, model, train_results, dev_results):\r\n",
    "  checkpoint_dir = \"checkpoints\"+f\"/{task_name}_{emb_name}\"\r\n",
    "  os.makedirs(checkpoint_dir, exist_ok=True)\r\n",
    "  model.save(checkpoint_dir)\r\n",
    "  results_fname = checkpoint_dir + '/results.json'\r\n",
    "  save_results(model, train_results, dev_results, results_fname)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "\r\n",
    "contradiction_labels = [\"negative\",\"contradict\",\"antonym\"]\r\n",
    "mono_labels = ['+', '-', '=']\r\n",
    "sentiment_labels = ['aligned', \"unaligned\", \"contradict\"]\r\n",
    "trainloader, devloader, n_classes, n_words = get_data_loaders(\"sentiment\", \"onehot\", 600, 64, sentiment_labels)\r\n",
    "model = get_model(n_classes, n_words, \"onehot\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "import os\r\n",
    "\r\n",
    "train(trainloader, devloader, model, 16, 2000)\r\n",
    "train_results, dev_results = eval_all(model, trainloader, devloader)\r\n",
    "save_checkpoints(\"sentiment\", \"onehot\", model, train_results, dev_results)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training loss: 0.0000 Dev loss: 0.5092 acc: 1.0000: 100%|██████████| 7712/7712 [00:09<00:00, 795.88it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Final loss. Train: 0.0 Dev: 0.44262760877609253\n",
      "Final acc. Train: 1.0 Dev: 1.0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "trainloader, devloader, n_classes, n_words = get_data_loaders(\"sentiment\", \"fasttext\", 600, 64, sentiment_labels)\r\n",
    "model = get_model(n_classes, n_words, \"onehot\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "import os\r\n",
    "train(trainloader, devloader, model, 100, 2000)\r\n",
    "train_results, dev_results = eval_all(model, trainloader, devloader)\r\n",
    "save_checkpoints(\"sentiment\", \"fasttext\", model, train_results, dev_results)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training loss: 1.2908 Dev loss: 1.6307 acc: 0.4492: 100%|██████████| 2100/2100 [00:03<00:00, 657.08it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Final loss. Train: 1.4222075309753417 Dev: 1.5225190416971843\n",
      "Final acc. Train: 0.5265000462532043 Dev: 0.4933333396911621\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def information_probe(loss, loss_clt, entropy):\r\n",
    "  gain = loss_clt - loss\r\n",
    "  return round(gain, 3), round(gain/entropy, 3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for loss in [0.211, 0.230, 0.231, 0.217, 0.167]:\r\n",
    "  onhot_info = information_probe(loss, 1.293, data_entropy)\r\n",
    "  fasttext_info = information_probe(loss, 1.264, data_entropy)\r\n",
    "  print((onhot_info, fasttext_info))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import util\r\n",
    "data_embeddings = util.read_data(\r\n",
    "                f\"./dataset/contradiction/output_fast_train\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for (sentence_emb, _) in data_embeddings:\r\n",
    "  print(sentence_emb[0].shape)\r\n",
    "  break"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "183bbf6827d058c2a2fb0f4acdc0420849dda2b4380af0e437e38c64d798d8b7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}