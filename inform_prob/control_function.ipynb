{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "import os \r\n",
    "import sys\r\n",
    "import torch\r\n",
    "import fasttext\r\n",
    "import fasttext.util"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "def load_fasttext():\r\n",
    "    ft_path = '../data/fasttext'\r\n",
    "    ft_fname = os.path.join(ft_path, 'cc.en.300.bin')\r\n",
    "    if not os.path.exists(ft_fname):\r\n",
    "        print(\"Downloading fasttext model\")\r\n",
    "        temp_fname = fasttext.util.download_model(\r\n",
    "            \"en\", if_exists='ignore')\r\n",
    "        os.rename(temp_fname, ft_fname)\r\n",
    "        os.rename(temp_fname + '.gz', ft_fname + '.gz')\r\n",
    "\r\n",
    "    print(\"Loading fasttext model\")\r\n",
    "    return fasttext.load_model(ft_fname)\r\n",
    "\r\n",
    "fasttext_model = load_fasttext()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading fasttext model\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import json\r\n",
    "\r\n",
    "train_data = []\r\n",
    "with open('/content/tasks/data/semgraph2/train.jsonl', 'r') as json_file:\r\n",
    "    json_list = list(json_file)\r\n",
    "    for json_str in json_list:\r\n",
    "      result = json.loads(json_str)\r\n",
    "      train_data.append(result)\r\n",
    "print(train_data[0]['text'])\r\n",
    "\r\n",
    "val_data = []\r\n",
    "with open('/content/tasks/data/semgraph2/val.jsonl', 'r') as json_file:\r\n",
    "    json_list = list(json_file)\r\n",
    "    for json_str in json_list:\r\n",
    "      result = json.loads(json_str)\r\n",
    "      val_data.append(result)\r\n",
    "print(val_data[0]['text'])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "A nun looking at a camera\n",
      "Polymeal nutrition increases cardiovascular mortality\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def flatten_list(_2d_list):\r\n",
    "    flat_list = []\r\n",
    "    # Iterate through the outer list\r\n",
    "    for element in _2d_list:\r\n",
    "        if type(element) is list:\r\n",
    "            # If the element is of type list, iterate through the sublist\r\n",
    "            for item in element:\r\n",
    "                flat_list.append(item)\r\n",
    "        else:\r\n",
    "            flat_list.append(element)\r\n",
    "    return flat_list\r\n",
    "\r\n",
    "import pandas as pd\r\n",
    "\r\n",
    "val_df = pd.DataFrame(val_data)\r\n",
    "val_targets = val_df['targets']\r\n",
    "val_targets = flatten_list(val_targets)\r\n",
    "val_y_df = pd.DataFrame(val_targets)\r\n",
    "\r\n",
    "val_y_df['label'].value_counts()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "no_relation            131006\n",
       "relation_2_relation     28158\n",
       "modifier_2_concept      26856\n",
       "concept_2_modifier      26856\n",
       "concept_2_relation      21449\n",
       "relation_2_concpet      21449\n",
       "relation_2_modifier      6170\n",
       "modifier_2_relation      6170\n",
       "Name: label, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "from math import log2\r\n",
    "\r\n",
    "def entropy(classes):\r\n",
    "  total = sum(classes)\r\n",
    "  entropy = 0.0\r\n",
    "  for cls in classes:\r\n",
    "    entropy += (cls/total) * log2(cls/total)\r\n",
    "  return -entropy\r\n",
    "\r\n",
    "data_entropy = entropy([28158,26856,26856,21449,21449,6170,6170])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import string\r\n",
    "\r\n",
    "def tokenize(text):\r\n",
    "  text.translate(str.maketrans('', '', string.punctuation))\r\n",
    "  return text.split()\r\n",
    "\r\n",
    "train_words = [tokenize(example['text']) for example in train_data]\r\n",
    "val_words = [tokenize(example['text']) for example in val_data]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def get_fasttext(fasttext_model, words):\r\n",
    "    embeddings = [[fasttext_model[word] for word in sentence] \r\n",
    "                  for sentence in words]\r\n",
    "    return embeddings\r\n",
    "\r\n",
    "train_fast_embeddings = get_fasttext(fasttext_model, train_words)\r\n",
    "val_fast_embeddings = get_fasttext(fasttext_model, val_words)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "output_fast_train = zip(train_fast_embeddings, train_words)\r\n",
    "output_fast_val = zip(val_fast_embeddings, val_words)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "import pickle\r\n",
    "\r\n",
    "with open(\"./dataset/semgraph2/output_fast_train\", \"wb\") as f:\r\n",
    "    pickle.dump(output_fast_train, f)\r\n",
    "with open(\"./dataset/semgraph2/output_fast_val\", \"wb\") as f:\r\n",
    "    pickle.dump(output_fast_val, f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "import torch\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "from dataset import SemgraphEdgeDataset\r\n",
    "from dataset import SemgraphNodeDataset\r\n",
    "\r\n",
    "from torch.utils.data import DataLoader\r\n",
    "\r\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
    "\r\n",
    "def get_data_cls(task):\r\n",
    "    if task == 'semgraph1':\r\n",
    "        return SemgraphNodeDataset\r\n",
    "    if task == 'semgraph2':\r\n",
    "        return SemgraphEdgeDataset\r\n",
    "\r\n",
    "\r\n",
    "def generate_batch(batch):\r\n",
    "    x = torch.cat([item[0].unsqueeze(0) for item in batch], dim=0)\r\n",
    "    y = torch.cat([item[1].unsqueeze(0) for item in batch], dim=0)\r\n",
    "\r\n",
    "    x, y = x.to(device), y.to(device, dtype=torch.long)\r\n",
    "    return (x, y)\r\n",
    "\r\n",
    "\r\n",
    "def get_data_loader(task, dataset_cls, representations,\r\n",
    "                    pca_size, mode, batch_size, shuffle,\r\n",
    "                    pca=None, classes=None, words=None):\r\n",
    "    data_set = dataset_cls(task, representations, pca_size,\r\n",
    "                          mode, pca=pca, classes=classes, words=words)\r\n",
    "    dataloader = DataLoader(data_set, batch_size=batch_size,\r\n",
    "                            shuffle=shuffle, collate_fn=generate_batch)\r\n",
    "    return dataloader, data_set.pca, data_set.classes, data_set.words\r\n",
    "\r\n",
    "labels = [\"concept_2_relation\",\r\n",
    "        \"concept_2_modifier\",\r\n",
    "        \"relation_2_concpet\",\r\n",
    "        \"relation_2_modifier\",\r\n",
    "        \"modifier_2_concept\",\r\n",
    "        \"modifier_2_relation\",\r\n",
    "        \"relation_2_relation\"]\r\n",
    "\r\n",
    "def get_data_loaders(task, representations, pca_size, batch_size):\r\n",
    "    dataset_cls = get_data_cls(task)\r\n",
    "\r\n",
    "    trainloader, pca, classes, words = get_data_loader(task,\r\n",
    "        dataset_cls, representations, pca_size,\r\n",
    "        'train', batch_size=batch_size, shuffle=True,  classes=np.array(labels))\r\n",
    "    \r\n",
    "    devloader, _, classes, words = get_data_loader(task,\r\n",
    "        dataset_cls, representations, pca_size,\r\n",
    "        'val', batch_size=batch_size, shuffle=False, pca=pca,\r\n",
    "        classes=np.array(labels), words=words)\r\n",
    "    \r\n",
    "    return trainloader, devloader, devloader.dataset.n_classes, devloader.dataset.n_words\r\n",
    "\r\n",
    "trainloader, devloader, n_classes, n_words = get_data_loaders(\"semgraph2\", \"onehot\", 600, 64)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "207104\n",
      "268022\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "from model import MLP, TransparentDataParallel\r\n",
    "\r\n",
    "def get_model(n_classes, n_words):\r\n",
    "    mlp = MLP(\r\n",
    "        \"semgraph_edge\", embedding_size=600, n_classes=n_classes, hidden_size=128,\r\n",
    "        nlayers=1, dropout=0.3, representation=\"onehot\", n_words=n_words)\r\n",
    "\r\n",
    "    if torch.cuda.device_count() > 1:\r\n",
    "        mlp = TransparentDataParallel(mlp)\r\n",
    "    return mlp.to(device)\r\n",
    "  \r\n",
    "model = get_model(n_classes, n_words)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "import torch.nn as nn\r\n",
    "import torch.optim as optim\r\n",
    "from tqdm import tqdm\r\n",
    "from train_info import TrainInfo\r\n",
    "\r\n",
    "def train(trainloader, devloader, model, eval_batches, wait_iterations):\r\n",
    "    optimizer = optim.Adam(model.parameters())\r\n",
    "    criterion = nn.CrossEntropyLoss().to(device=device)\r\n",
    "\r\n",
    "    with tqdm(total=wait_iterations) as pbar:\r\n",
    "        mode_train_info = TrainInfo(pbar, wait_iterations, eval_batches)\r\n",
    "        while not mode_train_info.finish:\r\n",
    "            train_epoch(trainloader, devloader, model,\r\n",
    "                        optimizer, criterion, mode_train_info)\r\n",
    "\r\n",
    "    model.recover_best()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "def _evaluate(evalloader, model):\r\n",
    "  #criterion = nn.CrossEntropyLoss().to(device=device)\r\n",
    "  dev_loss, dev_acc = 0, 0\r\n",
    "  for x, y in evalloader:\r\n",
    "    loss, acc = model.eval_batch(x, y)\r\n",
    "    dev_loss += loss\r\n",
    "    dev_acc += acc\r\n",
    "\r\n",
    "  n_instances = len(evalloader.dataset)\r\n",
    "  return {\r\n",
    "    'loss': dev_loss / n_instances,\r\n",
    "    'acc': dev_acc / n_instances\r\n",
    "  }\r\n",
    "\r\n",
    "def evaluate(evalloader, model):\r\n",
    "  model.eval()\r\n",
    "  with torch.no_grad():\r\n",
    "    result = _evaluate(evalloader, model)\r\n",
    "  model.train()\r\n",
    "  return result\r\n",
    "\r\n",
    "def train_epoch(trainloader, devloader, model, optimizer, criterion, mode_train_info):\r\n",
    "  for x, y in trainloader:\r\n",
    "    loss = model.train_batch(x, y, optimizer)\r\n",
    "    mode_train_info.new_batch(loss)\r\n",
    "\r\n",
    "    if mode_train_info.eval:\r\n",
    "      dev_results = evaluate(devloader, model)\r\n",
    "\r\n",
    "      if mode_train_info.is_best(dev_results):\r\n",
    "        model.set_best()\r\n",
    "      elif mode_train_info.finish:\r\n",
    "        mode_train_info.print_progress(dev_results)\r\n",
    "        return\r\n",
    "\r\n",
    "      mode_train_info.print_progress(dev_results)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "def eval_all(model, trainloader, devloader):\r\n",
    "  train_results = evaluate(trainloader, model)\r\n",
    "  dev_results = evaluate(devloader, model)\r\n",
    "  \r\n",
    "  train_loss= train_results['loss']\r\n",
    "  test_loss = dev_results['loss']\r\n",
    "  train_acc = train_results['acc'] \r\n",
    "  test_acc = dev_results['acc']\r\n",
    "\r\n",
    "  print(f'Final loss. Train: {train_loss} Dev: {test_loss}')\r\n",
    "  print(f'Final acc. Train: {train_acc} Dev: {test_acc}')\r\n",
    "  return train_results, dev_results\r\n",
    "\r\n",
    "\r\n",
    "def save_results(model, train_results, dev_results, results_fname):\r\n",
    "  results = {'n_classes': model.n_classes,\r\n",
    "             'embedding_size': model.embedding_size,\r\n",
    "             'hidden_size': model.hidden_size,\r\n",
    "             'nlayers': model.nlayers,\r\n",
    "             'dropout_p': model.dropout_p,\r\n",
    "             'train_loss': train_results['loss'],\r\n",
    "             'dev_loss': dev_results['loss'],\r\n",
    "             'train_acc': train_results['acc'],\r\n",
    "             'dev_acc': dev_results['acc'],\r\n",
    "            }\r\n",
    "  with open(results_fname, \"w\") as write_file:\r\n",
    "    json.dump(results, write_file, indent=4)\r\n",
    "\r\n",
    "def save_checkpoints(model, train_results, dev_results):\r\n",
    "  model.save(\"checkpoints\")\r\n",
    "  #results_fname = \"checkpoints\" + '/results.json'\r\n",
    "  #save_results(model, train_results, dev_results, results_fname)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "train(trainloader, devloader, model, 100, 2000)\r\n",
    "train_results, dev_results = eval_all(model, trainloader, devloader)\r\n",
    "save_checkpoints(model, train_results, dev_results)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training loss: 0.7385 Dev loss: 6.4748 acc: 0.1565: 100%|██████████| 2100/2100 [01:08<00:00, 30.87it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Final loss. Train: 1.2827091778643493 Dev: 3.612653796404231\n",
      "Final acc. Train: 0.6804793477058411 Dev: 0.1587369740009308\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "trainloader, devloader, n_classes, n_words = get_data_loaders(\"semgraph2\", \"fasttext\", 600, 64)\r\n",
    "model = get_model(n_classes, n_words)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "train(trainloader, devloader, model, 100, 2000)\r\n",
    "train_results, dev_results = eval_all(model, trainloader, devloader)\r\n",
    "save_checkpoints(model, train_results, dev_results)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training loss: 0.7815 Dev loss: 6.2844 acc: 0.1579: 100%|██████████| 2100/2100 [01:06<00:00, 31.39it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Final loss. Train: 1.2715235286575313 Dev: 3.858918787323612\n",
      "Final acc. Train: 0.6799675226211548 Dev: 0.13734693825244904\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def information_probe(loss, loss_clt, entropy):\r\n",
    "  gain = loss_clt - loss\r\n",
    "  return gain, gain/entropy"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "183bbf6827d058c2a2fb0f4acdc0420849dda2b4380af0e437e38c64d798d8b7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}