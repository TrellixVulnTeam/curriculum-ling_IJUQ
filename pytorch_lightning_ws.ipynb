{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from typing import Optional\n",
    "\n",
    "import datasets\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "from pytorch_lightning import LightningDataModule, LightningModule, Trainer\n",
    "\n",
    "AVAIL_GPUS = min(1, torch.cuda.device_count())\n",
    "\n",
    "\n",
    "class GLUEDataModule(LightningDataModule):\n",
    "\n",
    "    glue_task_num_labels = {\n",
    "        \"adversarial_nli_r3\": 3,\n",
    "        \"lexical\": 3,\n",
    "        \"boolean\": 3,\n",
    "        \"comparative\": 3,\n",
    "        \"conditional\": 3,\n",
    "        \"counting\": 3,\n",
    "        \"negation\": 3,\n",
    "        \"quantifier\": 3,\n",
    "        \"transitive\": 2,\n",
    "        \"hypernymy\": 2,\n",
    "        \"hyponymy\": 2,\n",
    "        \"ner\": 2,\n",
    "        \"verbcorner\": 2,\n",
    "        \"verbnet\": 2,\n",
    "        \"syntactic_alternation\": 2,\n",
    "        \"syntactic_variation\": 2,\n",
    "        \"monotonicity_infer\": 3,\n",
    "        \"syllogism\": 2,\n",
    "        \"coreference\": 3,\n",
    "        \"puns\": 3,\n",
    "        \"sentiment\": 2,\n",
    "        \"kg_relations\": 2,\n",
    "        \"context_align\": 3,\n",
    "        \"sprl\": 2,\n",
    "        \"atomic\": 3,\n",
    "        \"social_chem\": 3,\n",
    "        \"socialqa\": 3,\n",
    "        \"physicalqa\": 3,\n",
    "        \"logiqa\": 3,\n",
    "        \"ester\": 3,\n",
    "        \"cosmoqa\": 3,\n",
    "        \"drop\": 3,\n",
    "        \"entailment_tree\": 3,\n",
    "        \"proof_writer\": 2,\n",
    "        \"temporal\": 2,\n",
    "        \"spatial\": 3,\n",
    "        \"counterfactual\": 3\n",
    "    }\n",
    "\n",
    "    loader_columns = [\n",
    "        \"datasets_idx\",\n",
    "        \"input_ids\",\n",
    "        \"token_type_ids\",\n",
    "        \"attention_mask\",\n",
    "        \"start_positions\",\n",
    "        \"end_positions\",\n",
    "        \"labels\",\n",
    "    ]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name_or_path: str,\n",
    "        task_name: str = \"atomic\",\n",
    "        max_seq_length: int = 128,\n",
    "        train_batch_size: int = 8,\n",
    "        eval_batch_size: int = 16,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model_name_or_path = model_name_or_path\n",
    "        self.task_name = task_name\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.eval_batch_size = eval_batch_size\n",
    "\n",
    "        self.text_fields = ['premise', 'hypothesis']\n",
    "        self.num_labels = self.glue_task_num_labels[task_name]\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name_or_path, use_fast=True)\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        self.dataset = datasets.load_dataset(\"curriculum_load_dataset.py\", self.task_name)\n",
    "\n",
    "        for split in self.dataset.keys():\n",
    "            self.dataset[split] = self.dataset[split].map(\n",
    "                self.convert_to_features,\n",
    "                batched=True,\n",
    "                remove_columns=[\"label\"],\n",
    "            )\n",
    "            self.columns = [c for c in self.dataset[split].column_names if c in self.loader_columns]\n",
    "            self.dataset[split].set_format(type=\"torch\", columns=self.columns)\n",
    "\n",
    "        self.eval_splits = [x for x in self.dataset.keys() if \"validation\" in x]\n",
    "\n",
    "    def prepare_data(self):\n",
    "        datasets.load_dataset(\"curriculum_load_dataset.py\", self.task_name)\n",
    "        AutoTokenizer.from_pretrained(self.model_name_or_path, use_fast=True)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.dataset[\"train\"], batch_size=self.train_batch_size)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        if len(self.eval_splits) == 1:\n",
    "            return DataLoader(self.dataset[\"validation\"], batch_size=len(self.dataset[\"validation\"]))\n",
    "        elif len(self.eval_splits) > 1:\n",
    "            return [DataLoader(self.dataset[x], batch_size=self.eval_batch_size) for x in self.eval_splits]\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        if len(self.eval_splits) == 1:\n",
    "            return DataLoader(self.dataset[\"test\"], batch_size=self.eval_batch_size)\n",
    "        elif len(self.eval_splits) > 1:\n",
    "            return [DataLoader(self.dataset[x], batch_size=self.eval_batch_size) for x in self.eval_splits]\n",
    "\n",
    "    def convert_to_features(self, example_batch, indices=None):\n",
    "\n",
    "        # Either encode single sentence or sentence pairs\n",
    "        if len(self.text_fields) > 1:\n",
    "            texts_or_text_pairs = list(zip(example_batch[self.text_fields[0]], example_batch[self.text_fields[1]]))\n",
    "        else:\n",
    "            texts_or_text_pairs = example_batch[self.text_fields[0]]\n",
    "\n",
    "        # Tokenize the text/text pairs\n",
    "        features = self.tokenizer.batch_encode_plus(\n",
    "            texts_or_text_pairs, max_length=self.max_seq_length, pad_to_max_length=True, truncation=True\n",
    "        )\n",
    "\n",
    "        # Rename label to labels to make it easier to pass to model forward\n",
    "        features[\"labels\"] = example_batch[\"label\"]\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import jiant.utils.python.io as py_io\n",
    "from jiant.utils.zlog import ZLogger\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\", level=logging.INFO\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def init_log_writer(output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    return ZLogger(output_dir, overwrite=True)\n",
    "\n",
    "class GLUETransformer(LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name_or_path: str,\n",
    "        num_labels: int,\n",
    "        train_loader: DataLoader,\n",
    "        task_name: str,\n",
    "        learning_rate: float = 1e-5,\n",
    "        adam_epsilon: float = 1e-8,\n",
    "        warmup_steps: int = 1000,\n",
    "        weight_decay: float = 0.0,\n",
    "        train_batch_size: int = 8,\n",
    "        eval_batch_size: int = 16,\n",
    "        eval_splits: Optional[list] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "        self.log_writer = init_log_writer(f\"./train_dynamics/{task_name}/\")\n",
    "        self.train_loader = train_loader\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "        self.config = AutoConfig.from_pretrained(model_name_or_path, num_labels=num_labels)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name_or_path, config=self.config)\n",
    "        self.metric = datasets.load_metric(\n",
    "            \"glue\", 'mnli', experiment_id=datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "        )\n",
    "\n",
    "    def forward(self, **inputs):\n",
    "        return self.model(**inputs)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs = self(**batch)\n",
    "        difficult_loss = outputs[0]\n",
    "\n",
    "        #data_with_loss = zip(batch, difficult_loss)\n",
    "        #data_loss_dict = {}\n",
    "        #for (data, ls) in data_with_loss:\n",
    "        #    data_loss_dict[data] = ls\n",
    "        #data_loss_pdict = pqdict(data_loss_dict)\n",
    "        #ranked_batch = list(data_loss_pdict.popkeys())\n",
    "\n",
    "        return difficult_loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        outputs = self(**batch)\n",
    "        val_loss, logits = outputs[:2]\n",
    "\n",
    "        if self.hparams.num_labels >= 1:\n",
    "            preds = torch.argmax(logits, axis=1)\n",
    "        elif self.hparams.num_labels == 1:\n",
    "            preds = logits.squeeze()\n",
    "\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        return {\"loss\": val_loss, \"preds\": preds, \"labels\": labels}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        preds = torch.cat([x[\"preds\"] for x in outputs]).detach().cpu().numpy()\n",
    "        labels = torch.cat([x[\"labels\"] for x in outputs]).detach().cpu().numpy()\n",
    "        loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
    "        eval_metrics = self.metric.compute(\n",
    "            predictions=preds, references=labels\n",
    "        )\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log_dict(eval_metrics, prog_bar=True)\n",
    "        logger.info(f\"val_acc: {eval_metrics['accuracy']}\")\n",
    "        return loss\n",
    "\n",
    "    def setup(self, stage=None) -> None:\n",
    "        if stage != \"fit\":\n",
    "            return\n",
    "\n",
    "        # Calculate total steps\n",
    "        tb_size = self.hparams.train_batch_size * max(1, self.trainer.gpus)\n",
    "        ab_size = self.trainer.accumulate_grad_batches * float(self.trainer.max_epochs)\n",
    "        self.total_steps = (len(self.train_loader.dataset) // tb_size) // ab_size\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Prepare optimizer and schedule (linear warmup and decay)\"\"\"\n",
    "        model = self.model\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": self.hparams.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 1e-5,\n",
    "            },\n",
    "        ]\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=self.hparams.learning_rate, eps=self.hparams.adam_epsilon)\n",
    "\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=self.hparams.warmup_steps,\n",
    "            num_training_steps=self.total_steps,\n",
    "        )\n",
    "        scheduler = {\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-06 14:31:24,523 - WARNING - datasets.builder - Reusing dataset curriculum (C:\\Users\\Admin\\.cache\\huggingface\\datasets\\curriculum\\adversarial_nli_r3\\1.0.0\\07cfdb88a5efbd3541abf7395d378b0bddadfc24ba6a3303beecac61f51b15df)\n",
      "2022-01-06 14:31:24,614 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at C:\\Users\\Admin\\.cache\\huggingface\\datasets\\curriculum\\adversarial_nli_r3\\1.0.0\\07cfdb88a5efbd3541abf7395d378b0bddadfc24ba6a3303beecac61f51b15df\\cache-1f7e9230e5123cf2.arrow\n",
      "2022-01-06 14:31:24,708 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at C:\\Users\\Admin\\.cache\\huggingface\\datasets\\curriculum\\adversarial_nli_r3\\1.0.0\\07cfdb88a5efbd3541abf7395d378b0bddadfc24ba6a3303beecac61f51b15df\\cache-7795babccbe3d087.arrow\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "dm = GLUEDataModule(\n",
    "    model_name_or_path=\"roberta-large\",\n",
    "    task_name=\"adversarial_nli_r3\",\n",
    ")\n",
    "dm.setup(\"fit\")\n",
    "\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    dirpath=f\"./runs/{dm.task_name}/roberta-large/checkpoints/\",\n",
    "    filename=\"checkpoint_best\",\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    save_top_k=2\n",
    ")\n",
    "\n",
    "train_params = dict(\n",
    "    gpus=1,\n",
    "    max_epochs=2,\n",
    "    progress_bar_refresh_rate=1,\n",
    "    checkpoint_callback=True,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    logger=TensorBoardLogger(\n",
    "        os.path.join(\"./runs\", 'logs'),\n",
    "        name=f\"{dm.model_name_or_path}-{dm.task_name}\",\n",
    "        version='trial_1'\n",
    "    ),\n",
    "    precision=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:147: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.\n",
      "  rank_zero_deprecation(\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:90: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=1)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                             | Params\n",
      "-----------------------------------------------------------\n",
      "0 | model | RobertaForSequenceClassification | 355 M \n",
      "-----------------------------------------------------------\n",
      "355 M     Trainable params\n",
      "0         Non-trainable params\n",
      "355 M     Total params\n",
      "710.726   Total estimated model params size (MB)\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:631: UserWarning: Checkpoint directory C:\\Users\\Admin\\Desktop\\research\\curriculum-ling\\runs\\adversarial_nli_r3\\roberta-large\\checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:116: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check: 100%|██████████| 2/2 [00:00<00:00,  3.11it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-06 04:32:00,212 - INFO - __main__ - val_acc: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:116: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 1/20434 [00:00<2:08:02,  2.66it/s, loss=1.2, v_num=al_1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\optim\\lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|█████████▉| 20433/20434 [2:13:43<00:00,  2.55it/s, loss=1.23, v_num=al_1]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-06 06:45:44,001 - INFO - __main__ - val_acc: 0.335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|█████████▉| 20433/20434 [2:14:31<00:00,  2.53it/s, loss=1.23, v_num=al_1, val_loss=1.110, accuracy=0.335]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-06 09:00:28,970 - INFO - __main__ - val_acc: 0.335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 20434/20434 [2:14:46<00:00,  2.53it/s, loss=1.23, v_num=al_1, val_loss=1.110, accuracy=0.335]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-06 09:00:53,407 - INFO - __main__ - val_acc: 0.335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 VALIDATE RESULTS\n",
      "{'accuracy': 0.33500000834465027, 'val_loss': 1.1080350875854492}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'val_loss': 1.1080350875854492, 'accuracy': 0.33500000834465027}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "model = GLUETransformer(\n",
    "    model_name_or_path=\"roberta-large\",\n",
    "    num_labels=dm.num_labels,\n",
    "    eval_splits=dm.eval_splits,\n",
    "    task_name=dm.task_name,\n",
    "    train_loader=dm.train_dataloader()\n",
    ")\n",
    "\n",
    "trainer = Trainer(**train_params)\n",
    "trainer.fit(model, dm.train_dataloader(), dm.val_dataloader())\n",
    "trainer.validate(model, dm.val_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-06 14:31:38,483 - INFO - __main__ - val_acc: 0.335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 VALIDATE RESULTS\n",
      "{'accuracy': 0.33500000834465027, 'val_loss': 1.1080350875854492}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'val_loss': 1.1080350875854492, 'accuracy': 0.33500000834465027}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.validate(model, dm.val_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dm.val_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_model = model.load_from_checkpoint(\"./runs/adversarial_nli_r3/roberta-large/checkpoints/checkpoint_best.ckpt\")\n",
    "trainer = Trainer(**train_params)\n",
    "trainer.validate(validate_model, dm.val_dataloader())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:190: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Question ::\n",
      "Dustin, Milton, Louis, Bill, Roland, Dean, Tim, Micheal, Philip, Adrian, Eddie, Bradley, Andy, Lawrence, Edgar and Phillip have only visited Ecuador\n",
      "\n",
      "\n",
      "Paraphrased Questions :: \n",
      "0: Dustin, Milton, Louis, Bill, Roland, Dean, Tim, Micheal, Philip, Adrian, Eddie, Bradley, Andy, Lawrence, Edgar and Phillip have only been visited by Ecuador. They spent time with Cuban, Brazil, Turkey and Honduras.\n",
      "1: Dustin, Milton, Louis, Bill, Roland, Dean, Tim, Micheal, Philip, Adrian, Eddie, Bradley, Andy, Lawrence, Edgar and Phillip have only visited Ecuador.\n",
      "2: Dustin, Milton, Louis, Bill, Roland, Roland, Dean, Tim, Micheal, Philip, Adrian, Eddie, Bradley, Andy, Lawrence, Edgar and Phillip have only visited Ecuador. Other notable sights include Rome, Spain, Brazil, Austria, Portugal, Thailand, Vietnam, Austria, Germany, France, The Bahamas, Portugal.\n",
      "3: Dustin, Milton, Louis, Bill, Roland, Dean, Tim, Micheal, Philip, Adrian, Eddie, Bradley, Andy, Lawrence, Edgar, and Phillip have only visited the Philippines. They are looking for a place to stay for a week and have to make a trip to Nicaragua and then return to Chile.\n",
      "4: Dustin, Milton, Louis, Bill, Roland, Dean, Tim, Micheal, Philip, Adrian, Eddie, Bradley, Andy, Lawrence, Edgar have only visited Ecuador. Since Ecuador is the border state of Hawaii, the border is in Venezuela.\n",
      "5: Dustin, Milton, Louis, Bill, Roland, Roland, Tim, Micheal, Philip, Adrian, Eddie, Bradley, Andy, Lawrence, Edgar and Phillip have only visited Mexico, which is near Honduras. I am wondering why would they be staying in Ecuador for three days?\n",
      "6: Dustin, Milton, Louis, Bill, Roland, Dean, Tim, Micheal, Phillip, Adrian, Eddie, Bradley, Andy, Lawrence, Edgar and Phillip all visited Bolivia, Greece and Nicaragua in November.\n",
      "7: Dustin, Milton, Louis, Bill, Roland, Roland, Dean, Tim, Micheal, Philip, Adrian, Eddie, Bradley, Andy, Lawrence, Edgar and Phillip have visited the Red Sea, Jamaica, Jamaica, Jamaica, Vietnam and South Korea, among many other countries.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5ForConditionalGeneration,T5Tokenizer\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "  torch.manual_seed(seed)\n",
    "  if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained('ramsrigouthamg/t5_paraphraser')\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print (\"device \",device)\n",
    "model = model.to(device)\n",
    "\n",
    "sentence = \"Which course should I take to get started in data science?\"\n",
    "# sentence = \"What are the ingredients required to bake a perfect cake?\"\n",
    "# sentence = \"What is the best possible approach to learn aeronautical engineering?\"\n",
    "# sentence = \"Do apples taste better than oranges in general?\"\n",
    "sentence = \"Dustin, Milton, Louis, Bill, Roland, Dean, Tim, Micheal, Philip, Adrian, Eddie, Bradley, Andy, Lawrence, Edgar and Phillip have only visited Ecuador\"\n",
    "\n",
    "\n",
    "text =  \"paraphrase: \" + sentence + \" </s>\"\n",
    "\n",
    "\n",
    "max_len = 256\n",
    "\n",
    "encoding = tokenizer.encode_plus(text,pad_to_max_length=True, return_tensors=\"pt\")\n",
    "input_ids, attention_masks = encoding[\"input_ids\"].to(device), encoding[\"attention_mask\"].to(device)\n",
    "\n",
    "\n",
    "# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\n",
    "beam_outputs = model.generate(\n",
    "    input_ids=input_ids, attention_mask=attention_masks,\n",
    "    do_sample=True,\n",
    "    max_length=256,\n",
    "    top_k=120,\n",
    "    top_p=0.98,\n",
    "    early_stopping=True,\n",
    "    num_return_sequences=10\n",
    ")\n",
    "\n",
    "\n",
    "print (\"\\nOriginal Question ::\")\n",
    "print (sentence)\n",
    "print (\"\\n\")\n",
    "print (\"Paraphrased Questions :: \")\n",
    "final_outputs =[]\n",
    "for beam_output in beam_outputs:\n",
    "    sent = tokenizer.decode(beam_output, skip_special_tokens=True,clean_up_tokenization_spaces=True)\n",
    "    if sent.lower() != sentence.lower() and sent not in final_outputs:\n",
    "        final_outputs.append(sent)\n",
    "\n",
    "for i, final_output in enumerate(final_outputs):\n",
    "    print(\"{}: {}\".format(i, final_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-04 18:37:56,514\tWARNING function_runner.py:563 -- Function checkpointing is disabled. This may result in unexpected behavior when using checkpointing features or certain schedulers. To enable, set the train function arguments to be `func(config, checkpoint_dir=None)`.\n",
      "2022-01-04 18:37:56,678\tWARNING tune.py:574 -- Tune detects GPUs, but no trials are using GPUs. To enable trials to use GPUs, set tune.run(resources_per_trial={'gpu': 1}...) which allows Tune to expose 1 GPU to each trial. You can also override `Trainable.default_resource_request` if using the Trainable API.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-04 18:37:56 (running for 00:00:00.27)<br>Memory usage on this node: 18.7/31.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/24 CPUs, 0/1 GPUs, 0.0/8.73 GiB heap, 0.0/4.37 GiB objects<br>Result logdir: C:\\Users\\Admin\\ray_results\\training_function_2022-01-04_18-37-56<br>Number of trials: 3/3 (3 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">  alpha</th><th style=\"text-align: right;\">  beta</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>training_function_580e9_00000</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">  0.001</td><td style=\"text-align: right;\">     1</td></tr>\n",
       "<tr><td>training_function_580e9_00001</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">  0.01 </td><td style=\"text-align: right;\">     1</td></tr>\n",
       "<tr><td>training_function_580e9_00002</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">  0.1  </td><td style=\"text-align: right;\">     1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-04 18:37:57,016\tERROR syncer.py:75 -- Log sync requires rsync to be installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for training_function_580e9_00000:\n",
      "  date: 2022-01-04_18-37-58\n",
      "  done: false\n",
      "  experiment_id: 90066193acd745f6ad8d99d4df405cde\n",
      "  hostname: DESKTOP-UIUES8U\n",
      "  iterations_since_restore: 1\n",
      "  mean_loss: 10.1\n",
      "  neg_mean_loss: -10.1\n",
      "  node_ip: 127.0.0.1\n",
      "  pid: 33648\n",
      "  time_since_restore: 0.0\n",
      "  time_this_iter_s: 0.0\n",
      "  time_total_s: 0.0\n",
      "  timestamp: 1641339478\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 580e9_00000\n",
      "  \n",
      "Result for training_function_580e9_00002:\n",
      "  date: 2022-01-04_18-37-58\n",
      "  done: false\n",
      "  experiment_id: a3d9de9aa3f84369b2eeeb8315810dd0\n",
      "  hostname: DESKTOP-UIUES8U\n",
      "  iterations_since_restore: 1\n",
      "  mean_loss: 10.1\n",
      "  neg_mean_loss: -10.1\n",
      "  node_ip: 127.0.0.1\n",
      "  pid: 48184\n",
      "  time_since_restore: 0.0\n",
      "  time_this_iter_s: 0.0\n",
      "  time_total_s: 0.0\n",
      "  timestamp: 1641339478\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 580e9_00002\n",
      "  \n",
      "Result for training_function_580e9_00001:\n",
      "  date: 2022-01-04_18-37-58\n",
      "  done: false\n",
      "  experiment_id: a4f2ed638c06489ea8e98a913ce2b645\n",
      "  hostname: DESKTOP-UIUES8U\n",
      "  iterations_since_restore: 1\n",
      "  mean_loss: 10.1\n",
      "  neg_mean_loss: -10.1\n",
      "  node_ip: 127.0.0.1\n",
      "  pid: 48908\n",
      "  time_since_restore: 0.0\n",
      "  time_this_iter_s: 0.0\n",
      "  time_total_s: 0.0\n",
      "  timestamp: 1641339478\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 580e9_00001\n",
      "  \n",
      "Result for training_function_580e9_00002:\n",
      "  date: 2022-01-04_18-37-58\n",
      "  done: true\n",
      "  experiment_id: a3d9de9aa3f84369b2eeeb8315810dd0\n",
      "  experiment_tag: 2_alpha=0.1,beta=1\n",
      "  hostname: DESKTOP-UIUES8U\n",
      "  iterations_since_restore: 10\n",
      "  mean_loss: 9.274311926605503\n",
      "  neg_mean_loss: -9.274311926605503\n",
      "  node_ip: 127.0.0.1\n",
      "  pid: 48184\n",
      "  time_since_restore: 0.0449984073638916\n",
      "  time_this_iter_s: 0.004998922348022461\n",
      "  time_total_s: 0.0449984073638916\n",
      "  timestamp: 1641339478\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 10\n",
      "  trial_id: 580e9_00002\n",
      "  \n",
      "Result for training_function_580e9_00000:\n",
      "  date: 2022-01-04_18-37-58\n",
      "  done: true\n",
      "  experiment_id: 90066193acd745f6ad8d99d4df405cde\n",
      "  experiment_tag: 0_alpha=0.001,beta=1\n",
      "  hostname: DESKTOP-UIUES8U\n",
      "  iterations_since_restore: 10\n",
      "  mean_loss: 10.091008092716553\n",
      "  neg_mean_loss: -10.091008092716553\n",
      "  node_ip: 127.0.0.1\n",
      "  pid: 33648\n",
      "  time_since_restore: 0.04799962043762207\n",
      "  time_this_iter_s: 0.004000425338745117\n",
      "  time_total_s: 0.04799962043762207\n",
      "  timestamp: 1641339478\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 10\n",
      "  trial_id: 580e9_00000\n",
      "  \n",
      "Result for training_function_580e9_00001:\n",
      "  date: 2022-01-04_18-37-58\n",
      "  done: true\n",
      "  experiment_id: a4f2ed638c06489ea8e98a913ce2b645\n",
      "  experiment_tag: 1_alpha=0.01,beta=1\n",
      "  hostname: DESKTOP-UIUES8U\n",
      "  iterations_since_restore: 10\n",
      "  mean_loss: 10.010802775024777\n",
      "  neg_mean_loss: -10.010802775024777\n",
      "  node_ip: 127.0.0.1\n",
      "  pid: 48908\n",
      "  time_since_restore: 0.06800007820129395\n",
      "  time_this_iter_s: 0.00500035285949707\n",
      "  time_total_s: 0.06800007820129395\n",
      "  timestamp: 1641339478\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 10\n",
      "  trial_id: 580e9_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-04 18:37:58 (running for 00:00:01.78)<br>Memory usage on this node: 18.9/31.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/24 CPUs, 0/1 GPUs, 0.0/8.73 GiB heap, 0.0/4.37 GiB objects<br>Result logdir: C:\\Users\\Admin\\ray_results\\training_function_2022-01-04_18-37-56<br>Number of trials: 3/3 (3 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  alpha</th><th style=\"text-align: right;\">  beta</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  neg_mean_loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>training_function_580e9_00000</td><td>TERMINATED</td><td>127.0.0.1:33648</td><td style=\"text-align: right;\">  0.001</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">10.091  </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">       0.0479996</td><td style=\"text-align: right;\">      -10.091  </td></tr>\n",
       "<tr><td>training_function_580e9_00001</td><td>TERMINATED</td><td>127.0.0.1:48908</td><td style=\"text-align: right;\">  0.01 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">10.0108 </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">       0.0680001</td><td style=\"text-align: right;\">      -10.0108 </td></tr>\n",
       "<tr><td>training_function_580e9_00002</td><td>TERMINATED</td><td>127.0.0.1:48184</td><td style=\"text-align: right;\">  0.1  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\"> 9.27431</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">       0.0449984</td><td style=\"text-align: right;\">       -9.27431</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-04 18:37:58,566\tINFO tune.py:630 -- Total run time: 2.05 seconds (1.77 seconds for the tuning loop).\n",
      "\u001b[2m\u001b[36m(pid=33648)\u001b[0m Windows fatal exception: access violation\n",
      "\u001b[2m\u001b[36m(pid=33648)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best config:  {'alpha': 0.1, 'beta': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=48184)\u001b[0m Windows fatal exception: access violation\n",
      "\u001b[2m\u001b[36m(pid=48184)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=48908)\u001b[0m Windows fatal exception: access violation\n",
      "\u001b[2m\u001b[36m(pid=48908)\u001b[0m \n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\ray\\tune\\analysis\\experiment_analysis.py:565: UserWarning: Dataframes will use '/' instead of '.' to delimit nested result keys in future versions of Ray. For forward compatibility, set the environment variable TUNE_RESULT_DELIM='/'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from ray import tune\n",
    "\n",
    "\n",
    "def objective(step, alpha, beta):\n",
    "    return (0.1 + alpha * step / 100)**(-1) + beta * 0.1\n",
    "\n",
    "\n",
    "def training_function(config):\n",
    "    # Hyperparameters\n",
    "    alpha, beta = config[\"alpha\"], config[\"beta\"]\n",
    "    for step in range(10):\n",
    "        # Iterative training function - can be any arbitrary training procedure.\n",
    "        intermediate_score = objective(step, alpha, beta)\n",
    "        # Feed the score back back to Tune.\n",
    "        tune.report(mean_loss=intermediate_score)\n",
    "\n",
    "\n",
    "analysis = tune.run(\n",
    "    training_function,\n",
    "    config={\n",
    "        \"alpha\": tune.grid_search([0.001, 0.01, 0.1]),\n",
    "        \"beta\": tune.choice([1, 2, 3])\n",
    "    })\n",
    "\n",
    "print(\"Best config: \", analysis.get_best_config(\n",
    "    metric=\"mean_loss\", mode=\"min\"))\n",
    "\n",
    "# Get a dataframe for analyzing trial results.\n",
    "df = analysis.results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "183bbf6827d058c2a2fb0f4acdc0420849dda2b4380af0e437e38c64d798d8b7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
