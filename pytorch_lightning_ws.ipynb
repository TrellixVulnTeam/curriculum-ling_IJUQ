{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jiant.utils.python import io as py_io\n",
    "\n",
    "task_names = [\"factuality\", \"kg_relations\", \"megaveridicality\", \"ner\", \"puns\", \"sentiment\", \"verbcorner\", \"verbnet\", \"winogender\"]\n",
    "\n",
    "def make_curriculum_dataset(task_name):\n",
    "    train_data = py_io.read_jsonl(f\"./curriculum/{task_name}/train.jsonl\")\n",
    "    test_data = py_io.read_jsonl(f\"./curriculum/{task_name}/test.jsonl\")\n",
    "\n",
    "    train_examples = []\n",
    "    test_examples = []\n",
    "\n",
    "    for i, data in enumerate(train_data):\n",
    "        if task_name == \"kg_relations\":\n",
    "            label = data[\"binary-label\"]\n",
    "        else:\n",
    "            #label = data[\"label\"]\n",
    "            update_type = data['UpdateType']\n",
    "            if update_type == \"strengthener\":\n",
    "                label = \"entailed\"\n",
    "            else:\n",
    "                label = \"not-entailed\"\n",
    "        example = {\n",
    "            \"id\": i,\n",
    "            \"premise\": data[\"Update\"],\n",
    "            \"hypothesis\": data[\"Hypothesis\"],\n",
    "            \"gold_label\": label\n",
    "        }\n",
    "        train_examples.append(example)\n",
    "\n",
    "    for i, data in enumerate(test_data):\n",
    "        if task_name == \"kg_relations\":\n",
    "            label = data[\"binary-label\"]\n",
    "        else:\n",
    "            #label = data[\"label\"]\n",
    "            update_type = data['UpdateType']\n",
    "            if update_type == \"strengthener\":\n",
    "                label = \"entailed\"\n",
    "            else:\n",
    "                label = \"not-entailed\"\n",
    "        example = {\n",
    "            \"id\": i,\n",
    "            \"premise\": data[\"Update\"],\n",
    "            \"hypothesis\": data[\"Hypothesis\"],\n",
    "            \"gold_label\": label\n",
    "        }\n",
    "        test_examples.append(example)\n",
    "\n",
    "    os.makedirs(\"/content/tasks/configs/\", exist_ok=True)\n",
    "    os.makedirs(f\"/content/tasks/curriculum/{task_name}\", exist_ok=True)\n",
    "    py_io.write_jsonl(\n",
    "        data=train_examples, #train_darta,\n",
    "        path=f\"/content/tasks/curriculum/{task_name}/train.jsonl\",\n",
    "    )\n",
    "    py_io.write_jsonl(\n",
    "        data=test_examples,\n",
    "        path=f\"/content/tasks/curriculum/{task_name}/val.jsonl\",\n",
    "    )\n",
    "    py_io.write_json({\n",
    "    \"task\": f\"{task_name}\",\n",
    "    \"paths\": {\n",
    "        \"train\": f\"/content/tasks/curriculum/{task_name}/train.jsonl\",\n",
    "        \"val\": f\"/content/tasks/curriculum/{task_name}/val.jsonl\",\n",
    "    },\n",
    "    \"name\": f\"{task_name}\"\n",
    "    }, f\"/content/tasks/configs/{task_name}_config.json\")\n",
    "\n",
    "#for task_name in task_names:\n",
    "#    make_curriculum_dataset(task_name)\n",
    "#    print(f\"Build {task_name} dataset.\")\n",
    "make_curriculum_dataset(\"social_chem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_table('./curriculum/fava/combined/train.tsv')\n",
    "test_df = pd.read_table('./curriculum/fava/combined/test.tsv')\n",
    "dev_df = pd.read_table('./curriculum/fava/combined/dev.tsv')\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "premises = []\n",
    "hypothesiss = []\n",
    "\n",
    "count = 1\n",
    "for row in test_df.itertuples(index=False):\n",
    "  if count % 2 == 0:\n",
    "    hypothesiss.append(row)\n",
    "  else:\n",
    "    premises.append(row)\n",
    "  count += 1\n",
    "  #premise = row.premise\n",
    "  #sources = row.source.replace(\"[\",\"\").replace(\"]\",\"\").split(\"', \")\n",
    "  #targets = row.target.replace(\"[\",\"\").replace(\"]\",\"\").split(\"', \")\n",
    "  #relations = row.labels.replace(\"[\",\"\").replace(\"]\",\"\").split(\"', \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 1\n",
    "for row in dev_df.itertuples(index=False):\n",
    "  if count % 2 == 0:\n",
    "    hypothesiss.append(row)\n",
    "  else:\n",
    "    premises.append(row)\n",
    "  count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples = []\n",
    "\n",
    "for i in range(len(premises)):\n",
    "  premise_row = premises[i]\n",
    "  hypothesis_row = hypothesiss[i]\n",
    "\n",
    "  premise = premise_row.sentence\n",
    "  hypothesis = hypothesis_row.sentence\n",
    "  lable = hypothesis_row.label\n",
    "  if lable == 0:\n",
    "    gold_label = \"not-entailed\"\n",
    "  else:\n",
    "    gold_label = \"entailed\"\n",
    "\n",
    "  example = {\n",
    "      \"id\": i,\n",
    "      \"verb_frame\": premise_row.category,\n",
    "      \"premise\": premise,\n",
    "      \"hypothesis\": hypothesis,\n",
    "      \"gold_label\": gold_label\n",
    "  }\n",
    "  train_examples.append(example)\n",
    "\n",
    "py_io.write_jsonl(\n",
    "    data=train_examples,\n",
    "    path=f\"/content/tasks/curriculum/syntactic_alternation/val.jsonl\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name = \"syntactic_alternation\"\n",
    "\n",
    "py_io.write_json({\n",
    "    \"task\": f\"{task_name}\",\n",
    "    \"paths\": {\n",
    "        \"train\": f\"/content/tasks/curriculum/{task_name}/train.jsonl\",\n",
    "        \"val\": f\"/content/tasks/curriculum/{task_name}/val.jsonl\",\n",
    "    },\n",
    "    \"name\": f\"{task_name}\"\n",
    "    }, f\"/content/tasks/configs/{task_name}_config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_test(model, test_data):\n",
    "  premises = []\n",
    "  hypothesis = []\n",
    "  labels = []\n",
    "  for data in test_data:\n",
    "    premises.append(data[\"premise\"])\n",
    "    hypothesis.append(data[\"hypothesis\"])\n",
    "    labels.append(data[\"gold_label\"])\n",
    "\n",
    "  classes = [\"entailment\", \"contradiction\", \"neutral\"]\n",
    "  classes = [\"entailed\", \"not-entailed\"]\n",
    "\n",
    "  num_correct = 0\n",
    "  for i in tqdm(range(len(premises))):\n",
    "    test_sentence = tokenizer(premises[i], hypothesis[i], return_tensors=\"pt\")\n",
    "    test_sentence.to('cuda')\n",
    "    logits = model(**test_sentence).logits\n",
    "    out = torch.softmax(logits, dim=1)\n",
    "    pred = torch.argmax(out).cpu().numpy()\n",
    "    if pred == classes.index(labels[i]):\n",
    "      num_correct += 1\n",
    "\n",
    "  acc  = num_correct * 100 / len(premises)\n",
    "  return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "bert_base = \"textattack/bert-base-uncased-MNLI\"\n",
    "bert_large = \"sentence-transformers/bert-large-nli-mean-tokens\"\n",
    "roberta_base = \"textattack/roberta-base-MNLI\"\n",
    "roberta_large = \"roberta-large-mnli\"\n",
    "deberta_base = \"microsoft/deberta-base-mnli\"\n",
    "bart_large=\"textattack/facebook-bart-large-MNLI\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(roberta_large)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(roberta_large)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_names = [\"factuality\", \"kg_relations\", \"megaveridicality\", \"ner\", \"puns\", \"sentiment\", \"verbcorner\", \"verbnet\", \"winogender\"]\n",
    "semanitc_nli_eval = {}\n",
    "\n",
    "for task in task_names:\n",
    "  test_data = py_io.read_jsonl(f\"/content/tasks/curriculum/{task}/val.jsonl\")\n",
    "  acc = model_test(model, test_data)\n",
    "  semanitc_nli_eval[task] = acc\n",
    "\n",
    "py_io.write_json(semanitc_nli_eval, \"./curriculum/Semantic/roberta-large-eval.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semanitc_nli_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py_io.write_json(semanitc_nli_eval, \"./curriculum/semantic-roberta-large-eval.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_names = [\"boolean\", \"comparative\", \"conditional\", \"counting\", \"negation\", \"quantifier\", \"monotonicity_simple\", \"monotonicity_hard\"]\n",
    "logical_nli_eval = {}\n",
    "\n",
    "for task in task_names:\n",
    "  test_data = py_io.read_jsonl(f\"./curriculum/Logical/{task}/test/test.json\")\n",
    "  acc = model_test(model, test_data)\n",
    "  logical_nli_eval[task] = acc\n",
    "\n",
    "py_io.write_json(logical_nli_eval, \"./curriculum/Logical/bert-large-eval.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "definitions = py_io.read_jsonl(\"./curriculum/World/dictionary_qa/train.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(definitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def get_antonyms(input_lemma):\n",
    "    antonyms = []\n",
    "    for syn in wn.synsets(input_lemma):\n",
    "        for lemma in syn.lemmas():\n",
    "            if lemma.antonyms():\n",
    "                antonyms.append(lemma.antonyms()[0].name())\n",
    "    if len(antonyms) > 0:\n",
    "        return antonyms[0]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for syn in wn.synsets(\"separation\"):\n",
    "  print(syn.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def build_definition_example(question):\n",
    "  stem = question['question']['stem']\n",
    "  stem_ls = stem.split(\"'\")\n",
    "  premise = stem_ls[1]\n",
    "  surface_form = stem_ls[3]\n",
    "  #antonym = get_antonyms(surface_form)\n",
    "  answer_key = int(question['answerKey'])\n",
    "  keys = [0,1,2,3,4]\n",
    "  keys.remove(answer_key)\n",
    "  wront_key = random.choice(keys)\n",
    "  definition = question['question']['choices'][answer_key]['text']\n",
    "  irrelevent = question['question']['choices'][wront_key]['text']\n",
    "  hypothesis = premise.replace(surface_form, definition)\n",
    "  neutral = premise.replace(surface_form, irrelevent)\n",
    "  #if not antonym is None:\n",
    "  #  contradict = premise.replace(surface_form, wn.synsets(antonym)[0].definition())\n",
    "  #  return {\"premise\":premise, \"hypothesis\":hypothesis, \"neutral\":neutral, \"contradict\":contradict}\n",
    "  return {\"premise\":premise, \"hypothesis\":hypothesis, \"neutral\":neutral}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "definition_qa_nli = []\n",
    "\n",
    "i = 0\n",
    "for index, question in enumerate(definitions):\n",
    "  example = build_definition_example(question)\n",
    "  entail = {\"id\":i, \"sentence1\": example[\"premise\"], \"sentence2\": example[\"hypothesis\"], \"gold_label\": \"entailment\"}\n",
    "  definition_qa_nli.append(entail)\n",
    "  i += 1\n",
    "  neutral = {\"id\":i, \"sentence1\": example[\"premise\"], \"sentence2\": example[\"neutral\"], \"gold_label\": \"neutral\"}\n",
    "  definition_qa_nli.append(neutral)\n",
    "  i += 1\n",
    "  #if len( example[\"contradict\"]) > 0:\n",
    "  #  contradict = {\"id\":i, \"premise\": example[\"premise\"], \"hypothesis\": example[\"contradict\"], \"gold_label\": \"neutral\"}\n",
    "  #  definition_qa_nli.append(contradict)\n",
    "  #  i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py_io.write_jsonl(definition_qa_nli, \"./curriculum/dictionary_nli/train.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read OntoNotes Data Copra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp_models.common.ontonotes import Ontonotes, OntonotesSentence\n",
    "\n",
    "ontonotes_train_dataset = Ontonotes()\n",
    "train_iterator = ontonotes_train_dataset.dataset_iterator(\"./curriculum/wsj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_instances = list(ontonotes_train_dataset.read(\"./curriculum/wsj\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate K-shot Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from jiant.utils.python.io import read_jsonl, write_jsonl\n",
    "\n",
    "def load_curriculum_datasets(data_dir, tasks):\n",
    "    datasets = {}\n",
    "    for task in tasks:\n",
    "        dataset = {}\n",
    "        dirname = os.path.join(data_dir, task)\n",
    "        splits = [\"train\", \"val\"]\n",
    "        for split in splits:\n",
    "            filename = os.path.join(dirname, f\"{split}.jsonl\")\n",
    "            dataset[split] = read_jsonl(filename)\n",
    "        datasets[task] = dataset\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_dict = dict(\n",
    "    k=32,\n",
    "    task=[\"lexical_nli\", \"socialqa_nli\"],\n",
    "    seed=[100, 13, 21, 42, 87],\n",
    "    data_dir=\"/content/tasks/data\",\n",
    "    output_dir=\"./few_shot/\",\n",
    "    mode='k-shot', # k-shot-10x\n",
    ")\n",
    "\n",
    "args = argparse.Namespace(**args_dict)\n",
    "args.output_dir = os.path.join(args.output_dir, args.mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "k = args.k\n",
    "print(\"K =\", k)\n",
    "datasets = load_curriculum_datasets(args.data_dir, args.task)\n",
    "\n",
    "for seed in args.seed:\n",
    "    print(\"Seed = %d\" % (seed))\n",
    "    for task, dataset in datasets.items():\n",
    "        # Set random seed\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        # Shuffle the training set\n",
    "        print(\"| Task = %s\" % (task))\n",
    "        train_lines = dataset['train']\n",
    "        np.random.shuffle(train_lines)\n",
    "\n",
    "        # Set up dir\n",
    "        task_dir = os.path.join(args.output_dir, task)\n",
    "        setting_dir = os.path.join(task_dir, f\"{k}-{seed}\")\n",
    "        os.makedirs(setting_dir, exist_ok=True)\n",
    "\n",
    "        # Write test splits\n",
    "        write_jsonl(dataset['val'], os.path.join(\n",
    "            setting_dir, 'val.jsonl'))\n",
    "\n",
    "        # Get label list for balanced sampling\n",
    "        label_list = {}\n",
    "        for line in train_lines:\n",
    "            label = line['gold_label']\n",
    "            if label not in label_list:\n",
    "                label_list[label] = [line]\n",
    "            else:\n",
    "                label_list[label].append(line)\n",
    "\n",
    "        new_train = []\n",
    "        for label in label_list:\n",
    "            new_train += label_list[label][:k]\n",
    "        write_jsonl(new_train, os.path.join(\n",
    "            setting_dir, 'train.jsonl'))\n",
    "\n",
    "        new_dev = []\n",
    "        for label in label_list:\n",
    "            dev_rate = 11 if '10x' in args.mode else 2\n",
    "            for line in label_list[label][k:k*dev_rate]:\n",
    "                new_dev.append(line)\n",
    "        write_jsonl(new_dev, os.path.join(\n",
    "            setting_dir, 'dev.jsonl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from typing import Optional\n",
    "\n",
    "import datasets\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "from pytorch_lightning import LightningDataModule, LightningModule, Trainer\n",
    "\n",
    "AVAIL_GPUS = min(1, torch.cuda.device_count())\n",
    "\n",
    "\n",
    "class GLUEDataModule(LightningDataModule):\n",
    "\n",
    "    glue_task_num_labels = {\n",
    "        \"lexical\":3,\n",
    "        \"syntactic_alternation\": 2,\n",
    "        \"monotonicity_infer\": 2,\n",
    "        \"boolean\": 3,\n",
    "        \"comparative\": 3,\n",
    "        \"conditional\": 3,\n",
    "        \"counting\": 3,\n",
    "        \"negation\": 3,\n",
    "        \"quantifier\": 3,\n",
    "        \"monotonicity_simple\": 3,\n",
    "        \"monotonicity_hard\": 3,\n",
    "        \"factuality\": 2,\n",
    "        \"kg_relations\": 2,\n",
    "        \"megaveridicality\": 2,\n",
    "        \"ner\": 2,\n",
    "        \"puns\": 2,\n",
    "        \"sentiment\": 2,\n",
    "        \"verbcorner\": 2,\n",
    "        \"verbnet\": 2,\n",
    "        \"winogender: 2\"\n",
    "        \"coreference\": 2,\n",
    "        \"atomic\": 2,\n",
    "        \"socail_chem\": 2,\n",
    "        \"socialqa\": 2,\n",
    "        \"physicalqa\": 2,\n",
    "        \"context_align\": 2,\n",
    "    }\n",
    "\n",
    "    loader_columns = [\n",
    "        \"datasets_idx\",\n",
    "        \"input_ids\",\n",
    "        \"token_type_ids\",\n",
    "        \"attention_mask\",\n",
    "        \"start_positions\",\n",
    "        \"end_positions\",\n",
    "        \"labels\",\n",
    "    ]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name_or_path: str,\n",
    "        task_name: str = \"atomic\",\n",
    "        max_seq_length: int = 128,\n",
    "        train_batch_size: int = 32,\n",
    "        eval_batch_size: int = 32,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model_name_or_path = model_name_or_path\n",
    "        self.task_name = task_name\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.eval_batch_size = eval_batch_size\n",
    "\n",
    "        self.text_fields = ['premise', 'hypothesis']\n",
    "        self.num_labels = self.glue_task_num_labels[task_name]\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name_or_path, use_fast=True)\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        self.dataset = datasets.load_dataset(\"curriculum_load_dataset.py\", self.task_name)\n",
    "\n",
    "        for split in self.dataset.keys():\n",
    "            self.dataset[split] = self.dataset[split].map(\n",
    "                self.convert_to_features,\n",
    "                batched=True,\n",
    "                remove_columns=[\"label\"],\n",
    "            )\n",
    "            self.columns = [c for c in self.dataset[split].column_names if c in self.loader_columns]\n",
    "            self.dataset[split].set_format(type=\"torch\", columns=self.columns)\n",
    "\n",
    "        self.eval_splits = [x for x in self.dataset.keys() if \"validation\" in x]\n",
    "\n",
    "    def prepare_data(self):\n",
    "        datasets.load_dataset(\"curriculum_load_dataset.py\", self.task_name)\n",
    "        AutoTokenizer.from_pretrained(self.model_name_or_path, use_fast=True)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.dataset[\"train\"], batch_size=self.train_batch_size)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        if len(self.eval_splits) == 1:\n",
    "            return DataLoader(self.dataset[\"validation\"], batch_size=self.eval_batch_size)\n",
    "        elif len(self.eval_splits) > 1:\n",
    "            return [DataLoader(self.dataset[x], batch_size=self.eval_batch_size) for x in self.eval_splits]\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        if len(self.eval_splits) == 1:\n",
    "            return DataLoader(self.dataset[\"test\"], batch_size=self.eval_batch_size)\n",
    "        elif len(self.eval_splits) > 1:\n",
    "            return [DataLoader(self.dataset[x], batch_size=self.eval_batch_size) for x in self.eval_splits]\n",
    "\n",
    "    def convert_to_features(self, example_batch, indices=None):\n",
    "\n",
    "        # Either encode single sentence or sentence pairs\n",
    "        if len(self.text_fields) > 1:\n",
    "            texts_or_text_pairs = list(zip(example_batch[self.text_fields[0]], example_batch[self.text_fields[1]]))\n",
    "        else:\n",
    "            texts_or_text_pairs = example_batch[self.text_fields[0]]\n",
    "\n",
    "        # Tokenize the text/text pairs\n",
    "        features = self.tokenizer.batch_encode_plus(\n",
    "            texts_or_text_pairs, max_length=self.max_seq_length, pad_to_max_length=True, truncation=True\n",
    "        )\n",
    "\n",
    "        # Rename label to labels to make it easier to pass to model forward\n",
    "        features[\"labels\"] = example_batch[\"label\"]\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pqdict import pqdict\n",
    "\n",
    "class GLUETransformer(LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name_or_path: str,\n",
    "        num_labels: int,\n",
    "        train_loader: DataLoader,\n",
    "        task_name: str,\n",
    "        learning_rate: float = 1e-5,\n",
    "        adam_epsilon: float = 1e-8,\n",
    "        warmup_steps: int = 0,\n",
    "        weight_decay: float = 0.0,\n",
    "        train_batch_size: int = 8,\n",
    "        eval_batch_size: int = 16,\n",
    "        eval_splits: Optional[list] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "        self.train_loader = train_loader\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "        self.config = AutoConfig.from_pretrained(model_name_or_path, num_labels=num_labels)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name_or_path, config=self.config)\n",
    "        self.metric = datasets.load_metric(\n",
    "            \"glue\", 'mnli', experiment_id=datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "        )\n",
    "\n",
    "        step = 5\n",
    "        raise_rate = 0.2\n",
    "\n",
    "\n",
    "    def forward(self, **inputs):\n",
    "        return self.model(**inputs)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs = self(**batch)\n",
    "        difficult_loss = outputs[0]\n",
    "        data_with_loss = zip(batch, difficult_loss)\n",
    "        data_loss_dict = {}\n",
    "        for (data, ls) in data_with_loss:\n",
    "            data_loss_dict[data] = ls\n",
    "        data_loss_pdict = pqdict(data_loss_dict)\n",
    "        ranked_batch = list(data_loss_pdict.popkeys())\n",
    "\n",
    "        return difficult_loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        outputs = self(**batch)\n",
    "        val_loss, logits = outputs[:2]\n",
    "\n",
    "        if self.hparams.num_labels >= 1:\n",
    "            preds = torch.argmax(logits, axis=1)\n",
    "        elif self.hparams.num_labels == 1:\n",
    "            preds = logits.squeeze()\n",
    "\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        return {\"loss\": val_loss, \"preds\": preds, \"labels\": labels}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        preds = torch.cat([x[\"preds\"] for x in outputs]).detach().cpu().numpy()\n",
    "        labels = torch.cat([x[\"labels\"] for x in outputs]).detach().cpu().numpy()\n",
    "        loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log_dict(self.metric.compute(predictions=preds, references=labels), prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def setup(self, stage=None) -> None:\n",
    "        if stage != \"fit\":\n",
    "            return\n",
    "\n",
    "        # Calculate total steps\n",
    "        tb_size = self.hparams.train_batch_size * max(1, self.trainer.gpus)\n",
    "        ab_size = self.trainer.accumulate_grad_batches * float(self.trainer.max_epochs)\n",
    "        self.total_steps = (len(self.train_loader.dataset) // tb_size) // ab_size\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Prepare optimizer and schedule (linear warmup and decay)\"\"\"\n",
    "        model = self.model\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": self.hparams.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 1e-5,\n",
    "            },\n",
    "        ]\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=self.hparams.learning_rate, eps=self.hparams.adam_epsilon)\n",
    "\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=self.hparams.warmup_steps,\n",
    "            num_training_steps=self.total_steps,\n",
    "        )\n",
    "        scheduler = {\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'c', 'b']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pqdict import pqdict\n",
    "\n",
    "pq = pqdict({'a':3, 'b':10, 'c':8})\n",
    "\n",
    "list(pq.popkeys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset curriculum/context_align (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to C:\\Users\\Admin\\.cache\\huggingface\\datasets\\curriculum\\context_align\\1.0.0\\546cd813c08ed2b2471465ee3c2679b6e9d71927c250a97b6f8833e072fc38e0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset curriculum downloaded and prepared to C:\\Users\\Admin\\.cache\\huggingface\\datasets\\curriculum\\context_align\\1.0.0\\546cd813c08ed2b2471465ee3c2679b6e9d71927c250a97b6f8833e072fc38e0. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:00<?, ?ba/s]C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2212: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████| 15/15 [00:00<00:00, 21.56ba/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 13.29ba/s]\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "dm = GLUEDataModule(\n",
    "    model_name_or_path=\"roberta-base\",\n",
    "    task_name=\"context_align\",\n",
    ")\n",
    "dm.setup(\"fit\")\n",
    "\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    dirpath=f\"./runs/{dm.task_name}/roberta-base/checkpoints/\",\n",
    "    filename=\"checkpoint_best\",\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    save_top_k=2\n",
    ")\n",
    "\n",
    "train_params = dict(\n",
    "    gpus=1,\n",
    "    max_epochs=5,\n",
    "    progress_bar_refresh_rate=1,\n",
    "    checkpoint_callback=True,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    logger=TensorBoardLogger(\n",
    "        os.path.join(\"./runs\", 'logs'),\n",
    "        name=f\"{dm.model_name_or_path}-{dm.task_name}\",\n",
    "        version='trial_1')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:147: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.\n",
      "  rank_zero_deprecation(\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:90: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=1)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "Reusing dataset curriculum (C:\\Users\\Admin\\.cache\\huggingface\\datasets\\curriculum\\context_align\\1.0.0\\546cd813c08ed2b2471465ee3c2679b6e9d71927c250a97b6f8833e072fc38e0)\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\pytorch_lightning\\core\\datamodule.py:469: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.\n",
      "  rank_zero_deprecation(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                             | Params\n",
      "-----------------------------------------------------------\n",
      "0 | model | RobertaForSequenceClassification | 124 M \n",
      "-----------------------------------------------------------\n",
      "124 M     Trainable params\n",
      "0         Non-trainable params\n",
      "124 M     Total params\n",
      "498.589   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:110: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:110: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 494/494 [02:12<00:00,  3.74it/s, loss=2.7, v_num=al_1, val_loss=1.190, accuracy=0.604]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset curriculum (C:\\Users\\Admin\\.cache\\huggingface\\datasets\\curriculum\\context_align\\1.0.0\\546cd813c08ed2b2471465ee3c2679b6e9d71927c250a97b6f8833e072fc38e0)\n",
      "100%|██████████| 15/15 [00:00<00:00, 19.69ba/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 10.87ba/s]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 VALIDATE RESULTS\n",
      "{'accuracy': 0.6036446690559387, 'val_loss': 1.1908454895019531}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'val_loss': 1.1908454895019531, 'accuracy': 0.6036446690559387}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "model = GLUETransformer(\n",
    "    model_name_or_path=\"roberta-base\",\n",
    "    num_labels=dm.num_labels,\n",
    "    eval_splits=dm.eval_splits,\n",
    "    task_name=dm.task_name,\n",
    "    train_loader=dm.train_dataloader()\n",
    ")\n",
    "\n",
    "trainer = Trainer(**train_params)\n",
    "trainer.fit(model, dm)\n",
    "trainer.validate(model, dm.val_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 VALIDATE RESULTS\n",
      "{'accuracy': 0.6036446690559387, 'val_loss': 1.1908454895019531}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'val_loss': 1.1908454895019531, 'accuracy': 0.6036446690559387}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_model = GLUETransformer.load_from_checkpoint(\"./runs/context_align/roberta-base/checkpoints/checkpoint_best.ckpt\")\n",
    "trainer = Trainer(**train_params)\n",
    "trainer.validate(validate_model, dm.val_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:190: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Question ::\n",
      "Dustin, Milton, Louis, Bill, Roland, Dean, Tim, Micheal, Philip, Adrian, Eddie, Bradley, Andy, Lawrence, Edgar and Phillip have only visited Ecuador\n",
      "\n",
      "\n",
      "Paraphrased Questions :: \n",
      "0: Dustin, Milton, Louis, Bill, Roland, Dean, Tim, Micheal, Philip, Adrian, Eddie, Bradley, Andy, Lawrence, Edgar and Phillip have only been visited by Ecuador. They spent time with Cuban, Brazil, Turkey and Honduras.\n",
      "1: Dustin, Milton, Louis, Bill, Roland, Dean, Tim, Micheal, Philip, Adrian, Eddie, Bradley, Andy, Lawrence, Edgar and Phillip have only visited Ecuador.\n",
      "2: Dustin, Milton, Louis, Bill, Roland, Roland, Dean, Tim, Micheal, Philip, Adrian, Eddie, Bradley, Andy, Lawrence, Edgar and Phillip have only visited Ecuador. Other notable sights include Rome, Spain, Brazil, Austria, Portugal, Thailand, Vietnam, Austria, Germany, France, The Bahamas, Portugal.\n",
      "3: Dustin, Milton, Louis, Bill, Roland, Dean, Tim, Micheal, Philip, Adrian, Eddie, Bradley, Andy, Lawrence, Edgar, and Phillip have only visited the Philippines. They are looking for a place to stay for a week and have to make a trip to Nicaragua and then return to Chile.\n",
      "4: Dustin, Milton, Louis, Bill, Roland, Dean, Tim, Micheal, Philip, Adrian, Eddie, Bradley, Andy, Lawrence, Edgar have only visited Ecuador. Since Ecuador is the border state of Hawaii, the border is in Venezuela.\n",
      "5: Dustin, Milton, Louis, Bill, Roland, Roland, Tim, Micheal, Philip, Adrian, Eddie, Bradley, Andy, Lawrence, Edgar and Phillip have only visited Mexico, which is near Honduras. I am wondering why would they be staying in Ecuador for three days?\n",
      "6: Dustin, Milton, Louis, Bill, Roland, Dean, Tim, Micheal, Phillip, Adrian, Eddie, Bradley, Andy, Lawrence, Edgar and Phillip all visited Bolivia, Greece and Nicaragua in November.\n",
      "7: Dustin, Milton, Louis, Bill, Roland, Roland, Dean, Tim, Micheal, Philip, Adrian, Eddie, Bradley, Andy, Lawrence, Edgar and Phillip have visited the Red Sea, Jamaica, Jamaica, Jamaica, Vietnam and South Korea, among many other countries.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5ForConditionalGeneration,T5Tokenizer\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "  torch.manual_seed(seed)\n",
    "  if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained('ramsrigouthamg/t5_paraphraser')\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print (\"device \",device)\n",
    "model = model.to(device)\n",
    "\n",
    "sentence = \"Which course should I take to get started in data science?\"\n",
    "# sentence = \"What are the ingredients required to bake a perfect cake?\"\n",
    "# sentence = \"What is the best possible approach to learn aeronautical engineering?\"\n",
    "# sentence = \"Do apples taste better than oranges in general?\"\n",
    "sentence = \"Dustin, Milton, Louis, Bill, Roland, Dean, Tim, Micheal, Philip, Adrian, Eddie, Bradley, Andy, Lawrence, Edgar and Phillip have only visited Ecuador\"\n",
    "\n",
    "\n",
    "text =  \"paraphrase: \" + sentence + \" </s>\"\n",
    "\n",
    "\n",
    "max_len = 256\n",
    "\n",
    "encoding = tokenizer.encode_plus(text,pad_to_max_length=True, return_tensors=\"pt\")\n",
    "input_ids, attention_masks = encoding[\"input_ids\"].to(device), encoding[\"attention_mask\"].to(device)\n",
    "\n",
    "\n",
    "# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\n",
    "beam_outputs = model.generate(\n",
    "    input_ids=input_ids, attention_mask=attention_masks,\n",
    "    do_sample=True,\n",
    "    max_length=256,\n",
    "    top_k=120,\n",
    "    top_p=0.98,\n",
    "    early_stopping=True,\n",
    "    num_return_sequences=10\n",
    ")\n",
    "\n",
    "\n",
    "print (\"\\nOriginal Question ::\")\n",
    "print (sentence)\n",
    "print (\"\\n\")\n",
    "print (\"Paraphrased Questions :: \")\n",
    "final_outputs =[]\n",
    "for beam_output in beam_outputs:\n",
    "    sent = tokenizer.decode(beam_output, skip_special_tokens=True,clean_up_tokenization_spaces=True)\n",
    "    if sent.lower() != sentence.lower() and sent not in final_outputs:\n",
    "        final_outputs.append(sent)\n",
    "\n",
    "for i, final_output in enumerate(final_outputs):\n",
    "    print(\"{}: {}\".format(i, final_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "183bbf6827d058c2a2fb0f4acdc0420849dda2b4380af0e437e38c64d798d8b7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
