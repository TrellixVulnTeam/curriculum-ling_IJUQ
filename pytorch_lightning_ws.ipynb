{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate K-shot Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from jiant.utils.python.io import read_jsonl, write_jsonl\n",
    "\n",
    "def load_curriculum_datasets(data_dir, tasks):\n",
    "    datasets = {}\n",
    "    for task in tasks:\n",
    "        dataset = {}\n",
    "        dirname = os.path.join(data_dir, task)\n",
    "        splits = [\"train\", \"val\"]\n",
    "        for split in splits:\n",
    "            filename = os.path.join(dirname, f\"{split}.jsonl\")\n",
    "            dataset[split] = read_jsonl(filename)\n",
    "        datasets[task] = dataset\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_dict = dict(\n",
    "    k=32,\n",
    "    task=[\"lexical_nli\", \"socialqa_nli\"],\n",
    "    seed=[100, 13, 21, 42, 87],\n",
    "    data_dir=\"/content/tasks/data\",\n",
    "    output_dir=\"./few_shot/\",\n",
    "    mode='k-shot', # k-shot-10x\n",
    ")\n",
    "\n",
    "args = argparse.Namespace(**args_dict)\n",
    "args.output_dir = os.path.join(args.output_dir, args.mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "k = args.k\n",
    "print(\"K =\", k)\n",
    "datasets = load_curriculum_datasets(args.data_dir, args.task)\n",
    "\n",
    "for seed in args.seed:\n",
    "    print(\"Seed = %d\" % (seed))\n",
    "    for task, dataset in datasets.items():\n",
    "        # Set random seed\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        # Shuffle the training set\n",
    "        print(\"| Task = %s\" % (task))\n",
    "        train_lines = dataset['train']\n",
    "        np.random.shuffle(train_lines)\n",
    "\n",
    "        # Set up dir\n",
    "        task_dir = os.path.join(args.output_dir, task)\n",
    "        setting_dir = os.path.join(task_dir, f\"{k}-{seed}\")\n",
    "        os.makedirs(setting_dir, exist_ok=True)\n",
    "\n",
    "        # Write test splits\n",
    "        write_jsonl(dataset['val'], os.path.join(\n",
    "            setting_dir, 'val.jsonl'))\n",
    "\n",
    "        # Get label list for balanced sampling\n",
    "        label_list = {}\n",
    "        for line in train_lines:\n",
    "            label = line['gold_label']\n",
    "            if label not in label_list:\n",
    "                label_list[label] = [line]\n",
    "            else:\n",
    "                label_list[label].append(line)\n",
    "\n",
    "        new_train = []\n",
    "        for label in label_list:\n",
    "            new_train += label_list[label][:k]\n",
    "        write_jsonl(new_train, os.path.join(\n",
    "            setting_dir, 'train.jsonl'))\n",
    "\n",
    "        new_dev = []\n",
    "        for label in label_list:\n",
    "            dev_rate = 11 if '10x' in args.mode else 2\n",
    "            for line in label_list[label][k:k*dev_rate]:\n",
    "                new_dev.append(line)\n",
    "        write_jsonl(new_dev, os.path.join(\n",
    "            setting_dir, 'dev.jsonl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from typing import Optional\n",
    "\n",
    "import datasets\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "from pytorch_lightning import LightningDataModule, LightningModule, Trainer\n",
    "\n",
    "AVAIL_GPUS = min(1, torch.cuda.device_count())\n",
    "\n",
    "\n",
    "class GLUEDataModule(LightningDataModule):\n",
    "\n",
    "    glue_task_num_labels = {\n",
    "        \"lexical\": 3,\n",
    "        \"boolean\": 3,\n",
    "        \"comparative\": 3,\n",
    "        \"conditional\": 3,\n",
    "        \"counting\": 3,\n",
    "        \"negation\": 3,\n",
    "        \"quantifier\": 3,\n",
    "        \"transitive\": 2,\n",
    "        \"hypernymy\": 2,\n",
    "        \"hyponymy\": 2,\n",
    "        \"ner\": 2,\n",
    "        \"verbcorner\": 2,\n",
    "        \"verbnet\": 2,\n",
    "        \"syntactic_alternation\": 2,\n",
    "        \"syntactic_variation\": 2,\n",
    "        \"monotonicity_infer\": 3,\n",
    "        \"syllogism\": 2,\n",
    "        \"coreference\": 3,\n",
    "        \"puns\": 3,\n",
    "        \"sentiment\": 2,\n",
    "        \"kg_relations\": 2,\n",
    "        \"context_align\": 3,\n",
    "        \"sprl\": 2,\n",
    "        \"atomic\": 3,\n",
    "        \"social_chem\": 3,\n",
    "        \"socialqa\": 3,\n",
    "        \"physicalqa\": 3,\n",
    "        \"logiqa\": 3,\n",
    "        \"ester\": 3,\n",
    "        \"cosmoqa\": 3,\n",
    "        \"drop\": 3,\n",
    "        \"entailment_tree\": 3,\n",
    "        \"proof_writer\": 2,\n",
    "        \"temporal\": 2,\n",
    "        \"spatial\": 3,\n",
    "        \"counterfactual\": 3\n",
    "    }\n",
    "\n",
    "    loader_columns = [\n",
    "        \"datasets_idx\",\n",
    "        \"input_ids\",\n",
    "        \"token_type_ids\",\n",
    "        \"attention_mask\",\n",
    "        \"start_positions\",\n",
    "        \"end_positions\",\n",
    "        \"labels\",\n",
    "    ]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name_or_path: str,\n",
    "        task_name: str = \"atomic\",\n",
    "        max_seq_length: int = 128,\n",
    "        train_batch_size: int = 32,\n",
    "        eval_batch_size: int = 32,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model_name_or_path = model_name_or_path\n",
    "        self.task_name = task_name\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.eval_batch_size = eval_batch_size\n",
    "\n",
    "        self.text_fields = ['premise', 'hypothesis']\n",
    "        self.num_labels = self.glue_task_num_labels[task_name]\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name_or_path, use_fast=True)\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        self.dataset = datasets.load_dataset(\"curriculum_load_dataset.py\", self.task_name)\n",
    "\n",
    "        for split in self.dataset.keys():\n",
    "            self.dataset[split] = self.dataset[split].map(\n",
    "                self.convert_to_features,\n",
    "                batched=True,\n",
    "                remove_columns=[\"label\"],\n",
    "            )\n",
    "            self.columns = [c for c in self.dataset[split].column_names if c in self.loader_columns]\n",
    "            self.dataset[split].set_format(type=\"torch\", columns=self.columns)\n",
    "\n",
    "        self.eval_splits = [x for x in self.dataset.keys() if \"validation\" in x]\n",
    "\n",
    "    def prepare_data(self):\n",
    "        datasets.load_dataset(\"curriculum_load_dataset.py\", self.task_name)\n",
    "        AutoTokenizer.from_pretrained(self.model_name_or_path, use_fast=True)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.dataset[\"train\"], batch_size=self.train_batch_size)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        if len(self.eval_splits) == 1:\n",
    "            return DataLoader(self.dataset[\"validation\"], batch_size=self.eval_batch_size)\n",
    "        elif len(self.eval_splits) > 1:\n",
    "            return [DataLoader(self.dataset[x], batch_size=self.eval_batch_size) for x in self.eval_splits]\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        if len(self.eval_splits) == 1:\n",
    "            return DataLoader(self.dataset[\"test\"], batch_size=self.eval_batch_size)\n",
    "        elif len(self.eval_splits) > 1:\n",
    "            return [DataLoader(self.dataset[x], batch_size=self.eval_batch_size) for x in self.eval_splits]\n",
    "\n",
    "    def convert_to_features(self, example_batch, indices=None):\n",
    "\n",
    "        # Either encode single sentence or sentence pairs\n",
    "        if len(self.text_fields) > 1:\n",
    "            texts_or_text_pairs = list(zip(example_batch[self.text_fields[0]], example_batch[self.text_fields[1]]))\n",
    "        else:\n",
    "            texts_or_text_pairs = example_batch[self.text_fields[0]]\n",
    "\n",
    "        # Tokenize the text/text pairs\n",
    "        features = self.tokenizer.batch_encode_plus(\n",
    "            texts_or_text_pairs, max_length=self.max_seq_length, pad_to_max_length=True, truncation=True\n",
    "        )\n",
    "\n",
    "        # Rename label to labels to make it easier to pass to model forward\n",
    "        features[\"labels\"] = example_batch[\"label\"]\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pqdict import pqdict\n",
    "import jiant.utils.python.io as py_io\n",
    "from jiant.utils.zlog import ZLogger\n",
    "\n",
    "def init_log_writer(output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    return ZLogger(output_dir, overwrite=True)\n",
    "\n",
    "class GLUETransformer(LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name_or_path: str,\n",
    "        num_labels: int,\n",
    "        train_loader: DataLoader,\n",
    "        task_name: str,\n",
    "        learning_rate: float = 1e-5,\n",
    "        adam_epsilon: float = 1e-8,\n",
    "        warmup_steps: int = 0,\n",
    "        weight_decay: float = 0.0,\n",
    "        train_batch_size: int = 8,\n",
    "        eval_batch_size: int = 16,\n",
    "        eval_splits: Optional[list] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "        #self.current_epoch = 0\n",
    "        #self.log_writer = init_log_writer(f\"./train_dynamics/{task_name}/\")\n",
    "        self.train_loader = train_loader\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "        self.config = AutoConfig.from_pretrained(model_name_or_path, num_labels=num_labels)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name_or_path, config=self.config)\n",
    "        self.metric = datasets.load_metric(\n",
    "            \"glue\", 'mnli', experiment_id=datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "        )\n",
    "\n",
    "        step = 5\n",
    "        raise_rate = 0.2\n",
    "\n",
    "\n",
    "    def forward(self, **inputs):\n",
    "        return self.model(**inputs)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs = self(**batch)\n",
    "        difficult_loss = outputs[0]\n",
    "\n",
    "        #data_with_loss = zip(batch, difficult_loss)\n",
    "        #data_loss_dict = {}\n",
    "        #for (data, ls) in data_with_loss:\n",
    "        #    data_loss_dict[data] = ls\n",
    "        #data_loss_pdict = pqdict(data_loss_dict)\n",
    "        #ranked_batch = list(data_loss_pdict.popkeys())\n",
    "\n",
    "        return difficult_loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        outputs = self(**batch)\n",
    "        val_loss, logits = outputs[:2]\n",
    "\n",
    "        if self.hparams.num_labels >= 1:\n",
    "            preds = torch.argmax(logits, axis=1)\n",
    "        elif self.hparams.num_labels == 1:\n",
    "            preds = logits.squeeze()\n",
    "\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        return {\"loss\": val_loss, \"preds\": preds, \"labels\": labels}\n",
    "\n",
    "    #def training_epoch_end(self, outputs):\n",
    "    #    self.current_epoch += 1\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        preds = torch.cat([x[\"preds\"] for x in outputs]).detach().cpu().numpy()\n",
    "        labels = torch.cat([x[\"labels\"] for x in outputs]).detach().cpu().numpy()\n",
    "        loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log_dict(self.metric.compute(predictions=preds, references=labels), prog_bar=True)\n",
    "        val_loss_record = py_io.read_json(\"./runs/zero-shot/anli_roberta/inoculation_base_loss.json\")\n",
    "        val_loss_record[self.hparams.task_name] = loss.item()\n",
    "        py_io.write_json(val_loss_record, \"./runs/zero-shot/anli_roberta/inoculation_base_loss.json\")\n",
    "        return loss\n",
    "\n",
    "    def setup(self, stage=None) -> None:\n",
    "        if stage != \"fit\":\n",
    "            return\n",
    "\n",
    "        # Calculate total steps\n",
    "        tb_size = self.hparams.train_batch_size * max(1, self.trainer.gpus)\n",
    "        ab_size = self.trainer.accumulate_grad_batches * float(self.trainer.max_epochs)\n",
    "        self.total_steps = (len(self.train_loader.dataset) // tb_size) // ab_size\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Prepare optimizer and schedule (linear warmup and decay)\"\"\"\n",
    "        model = self.model\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": self.hparams.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 1e-5,\n",
    "            },\n",
    "        ]\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=self.hparams.learning_rate, eps=self.hparams.adam_epsilon)\n",
    "\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=self.hparams.warmup_steps,\n",
    "            num_training_steps=self.total_steps,\n",
    "        )\n",
    "        scheduler = {\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset curriculum/drop (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to C:\\Users\\Admin\\.cache\\huggingface\\datasets\\curriculum\\drop\\1.0.0\\7aab0b873a95ffa65a2bb4ba2c1ccbc5b2e1d1a89a4fd6c0e8613e7ce79f7465...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset curriculum downloaded and prepared to C:\\Users\\Admin\\.cache\\huggingface\\datasets\\curriculum\\drop\\1.0.0\\7aab0b873a95ffa65a2bb4ba2c1ccbc5b2e1d1a89a4fd6c0e8613e7ce79f7465. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:02<00:00,  7.27ba/s]\n",
      "100%|██████████| 12/12 [00:01<00:00,  9.42ba/s]\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "dm = GLUEDataModule(\n",
    "    model_name_or_path=\"ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli\",\n",
    "    task_name=\"drop\",\n",
    ")\n",
    "dm.setup(\"fit\")\n",
    "\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    dirpath=f\"./runs/{dm.task_name}/roberta-base/checkpoints/\",\n",
    "    filename=\"checkpoint_best\",\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    save_top_k=2\n",
    ")\n",
    "\n",
    "train_params = dict(\n",
    "    gpus=1,\n",
    "    max_epochs=5,\n",
    "    progress_bar_refresh_rate=1,\n",
    "    checkpoint_callback=True,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    logger=TensorBoardLogger(\n",
    "        os.path.join(\"./runs\", 'logs'),\n",
    "        name=f\"{dm.model_name_or_path}-{dm.task_name}\",\n",
    "        version='trial_1')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 352/352 [01:20<00:00,  4.55it/s]--------------------------------------------------------------------------------\n",
      "DATALOADER:0 VALIDATE RESULTS\n",
      "{'accuracy': 0.4816098213195801, 'val_loss': 3.045374870300293}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'val_loss': 3.045374870300293, 'accuracy': 0.4816098213195801}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "model = GLUETransformer(\n",
    "    model_name_or_path=\"ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli\",\n",
    "    num_labels=dm.num_labels,\n",
    "    eval_splits=dm.eval_splits,\n",
    "    task_name=dm.task_name,\n",
    "    train_loader=dm.train_dataloader()\n",
    ")\n",
    "\n",
    "trainer = Trainer(**train_params)\n",
    "#trainer.fit(model, dm)\n",
    "trainer.validate(model, dm.val_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 VALIDATE RESULTS\n",
      "{'accuracy': 0.6036446690559387, 'val_loss': 1.1908454895019531}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'val_loss': 1.1908454895019531, 'accuracy': 0.6036446690559387}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_model = GLUETransformer.load_from_checkpoint(\"./runs/context_align/roberta-base/checkpoints/checkpoint_best.ckpt\")\n",
    "trainer = Trainer(**train_params)\n",
    "trainer.validate(validate_model, dm.val_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:190: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Question ::\n",
      "Dustin, Milton, Louis, Bill, Roland, Dean, Tim, Micheal, Philip, Adrian, Eddie, Bradley, Andy, Lawrence, Edgar and Phillip have only visited Ecuador\n",
      "\n",
      "\n",
      "Paraphrased Questions :: \n",
      "0: Dustin, Milton, Louis, Bill, Roland, Dean, Tim, Micheal, Philip, Adrian, Eddie, Bradley, Andy, Lawrence, Edgar and Phillip have only been visited by Ecuador. They spent time with Cuban, Brazil, Turkey and Honduras.\n",
      "1: Dustin, Milton, Louis, Bill, Roland, Dean, Tim, Micheal, Philip, Adrian, Eddie, Bradley, Andy, Lawrence, Edgar and Phillip have only visited Ecuador.\n",
      "2: Dustin, Milton, Louis, Bill, Roland, Roland, Dean, Tim, Micheal, Philip, Adrian, Eddie, Bradley, Andy, Lawrence, Edgar and Phillip have only visited Ecuador. Other notable sights include Rome, Spain, Brazil, Austria, Portugal, Thailand, Vietnam, Austria, Germany, France, The Bahamas, Portugal.\n",
      "3: Dustin, Milton, Louis, Bill, Roland, Dean, Tim, Micheal, Philip, Adrian, Eddie, Bradley, Andy, Lawrence, Edgar, and Phillip have only visited the Philippines. They are looking for a place to stay for a week and have to make a trip to Nicaragua and then return to Chile.\n",
      "4: Dustin, Milton, Louis, Bill, Roland, Dean, Tim, Micheal, Philip, Adrian, Eddie, Bradley, Andy, Lawrence, Edgar have only visited Ecuador. Since Ecuador is the border state of Hawaii, the border is in Venezuela.\n",
      "5: Dustin, Milton, Louis, Bill, Roland, Roland, Tim, Micheal, Philip, Adrian, Eddie, Bradley, Andy, Lawrence, Edgar and Phillip have only visited Mexico, which is near Honduras. I am wondering why would they be staying in Ecuador for three days?\n",
      "6: Dustin, Milton, Louis, Bill, Roland, Dean, Tim, Micheal, Phillip, Adrian, Eddie, Bradley, Andy, Lawrence, Edgar and Phillip all visited Bolivia, Greece and Nicaragua in November.\n",
      "7: Dustin, Milton, Louis, Bill, Roland, Roland, Dean, Tim, Micheal, Philip, Adrian, Eddie, Bradley, Andy, Lawrence, Edgar and Phillip have visited the Red Sea, Jamaica, Jamaica, Jamaica, Vietnam and South Korea, among many other countries.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5ForConditionalGeneration,T5Tokenizer\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "  torch.manual_seed(seed)\n",
    "  if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained('ramsrigouthamg/t5_paraphraser')\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print (\"device \",device)\n",
    "model = model.to(device)\n",
    "\n",
    "sentence = \"Which course should I take to get started in data science?\"\n",
    "# sentence = \"What are the ingredients required to bake a perfect cake?\"\n",
    "# sentence = \"What is the best possible approach to learn aeronautical engineering?\"\n",
    "# sentence = \"Do apples taste better than oranges in general?\"\n",
    "sentence = \"Dustin, Milton, Louis, Bill, Roland, Dean, Tim, Micheal, Philip, Adrian, Eddie, Bradley, Andy, Lawrence, Edgar and Phillip have only visited Ecuador\"\n",
    "\n",
    "\n",
    "text =  \"paraphrase: \" + sentence + \" </s>\"\n",
    "\n",
    "\n",
    "max_len = 256\n",
    "\n",
    "encoding = tokenizer.encode_plus(text,pad_to_max_length=True, return_tensors=\"pt\")\n",
    "input_ids, attention_masks = encoding[\"input_ids\"].to(device), encoding[\"attention_mask\"].to(device)\n",
    "\n",
    "\n",
    "# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\n",
    "beam_outputs = model.generate(\n",
    "    input_ids=input_ids, attention_mask=attention_masks,\n",
    "    do_sample=True,\n",
    "    max_length=256,\n",
    "    top_k=120,\n",
    "    top_p=0.98,\n",
    "    early_stopping=True,\n",
    "    num_return_sequences=10\n",
    ")\n",
    "\n",
    "\n",
    "print (\"\\nOriginal Question ::\")\n",
    "print (sentence)\n",
    "print (\"\\n\")\n",
    "print (\"Paraphrased Questions :: \")\n",
    "final_outputs =[]\n",
    "for beam_output in beam_outputs:\n",
    "    sent = tokenizer.decode(beam_output, skip_special_tokens=True,clean_up_tokenization_spaces=True)\n",
    "    if sent.lower() != sentence.lower() and sent not in final_outputs:\n",
    "        final_outputs.append(sent)\n",
    "\n",
    "for i, final_output in enumerate(final_outputs):\n",
    "    print(\"{}: {}\".format(i, final_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "183bbf6827d058c2a2fb0f4acdc0420849dda2b4380af0e437e38c64d798d8b7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
