{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import os \r\n",
    "import sys\r\n",
    "import torch\r\n",
    "import fasttext\r\n",
    "import fasttext.util"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def load_fasttext():\r\n",
    "    ft_path = '../data/fasttext'\r\n",
    "    ft_fname = os.path.join(ft_path, 'cc.en.300.bin')\r\n",
    "    if not os.path.exists(ft_fname):\r\n",
    "        print(\"Downloading fasttext model\")\r\n",
    "        temp_fname = fasttext.util.download_model(\r\n",
    "            \"en\", if_exists='ignore')\r\n",
    "        os.rename(temp_fname, ft_fname)\r\n",
    "        os.rename(temp_fname + '.gz', ft_fname + '.gz')\r\n",
    "\r\n",
    "    print(\"Loading fasttext model\")\r\n",
    "    return fasttext.load_model(ft_fname)\r\n",
    "\r\n",
    "fasttext_model = load_fasttext()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading fasttext model\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import json\r\n",
    "\r\n",
    "train_data = []\r\n",
    "with open('/content/tasks/data/semgraph2/train.jsonl', 'r') as json_file:\r\n",
    "    json_list = list(json_file)\r\n",
    "    for json_str in json_list:\r\n",
    "      result = json.loads(json_str)\r\n",
    "      train_data.append(result)\r\n",
    "print(train_data[0]['text'])\r\n",
    "\r\n",
    "val_data = []\r\n",
    "with open('/content/tasks/data/semgraph2/val.jsonl', 'r') as json_file:\r\n",
    "    json_list = list(json_file)\r\n",
    "    for json_str in json_list:\r\n",
    "      result = json.loads(json_str)\r\n",
    "      val_data.append(result)\r\n",
    "print(val_data[0]['text'])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "A man is on a surfboard surfing the waves and catching good air\n",
      "Polymeal nutrition increases cardiovascular mortality\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "import string\r\n",
    "\r\n",
    "def tokenize(text):\r\n",
    "  text.translate(str.maketrans('', '', string.punctuation))\r\n",
    "  return text.split()\r\n",
    "\r\n",
    "train_words = [tokenize(example['text']) for example in train_data]\r\n",
    "val_words = [tokenize(example['text']) for example in val_data]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def get_fasttext(fasttext_model, words):\r\n",
    "    embeddings = [[fasttext_model[word] for word in sentence] \r\n",
    "                  for sentence in words]\r\n",
    "    return embeddings\r\n",
    "\r\n",
    "train_fast_embeddings = get_fasttext(fasttext_model, train_words)\r\n",
    "val_fast_embeddings = get_fasttext(fasttext_model, val_words)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "output_fast_train = zip(train_fast_embeddings, train_words)\r\n",
    "output_fast_val = zip(val_fast_embeddings, val_words)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "import pickle\r\n",
    "\r\n",
    "with open(\"./dataset/output_fast_train\", \"wb\") as f:\r\n",
    "    pickle.dump(output_fast_train, f)\r\n",
    "with open(\"./dataset/output_fast_val\", \"wb\") as f:\r\n",
    "    pickle.dump(output_fast_val, f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import torch\r\n",
    "\r\n",
    "from dataset import SemgraphEdgeDataset\r\n",
    "from dataset import SemgraphNodeDataset\r\n",
    "\r\n",
    "from torch.utils.data import DataLoader\r\n",
    "\r\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
    "\r\n",
    "def get_data_cls(task):\r\n",
    "    if task == 'semgraph_node':\r\n",
    "        return SemgraphNodeDataset\r\n",
    "    if task == 'semgraph_edge':\r\n",
    "        return SemgraphEdgeDataset\r\n",
    "\r\n",
    "\r\n",
    "def generate_batch(batch):\r\n",
    "    x = torch.cat([item[0].unsqueeze(0) for item in batch], dim=0)\r\n",
    "    y = torch.cat([item[1].unsqueeze(0) for item in batch], dim=0)\r\n",
    "\r\n",
    "    x, y = x.to(device), y.to(device, dtype=torch.long)\r\n",
    "    return (x, y)\r\n",
    "\r\n",
    "\r\n",
    "def get_data_loader(dataset_cls, representations,\r\n",
    "                    pca_size, mode, batch_size, shuffle,\r\n",
    "                    pca=None, classes=None, words=None):\r\n",
    "    data_set = dataset_cls(representations, pca_size,\r\n",
    "                          mode, pca=pca, classes=classes, words=words)\r\n",
    "    dataloader = DataLoader(data_set, batch_size=batch_size,\r\n",
    "                            shuffle=shuffle, collate_fn=generate_batch)\r\n",
    "    print(data_set.words)\r\n",
    "    return dataloader, data_set.pca, data_set.classes, data_set.words\r\n",
    "\r\n",
    "\r\n",
    "def get_data_loaders(task, representations, pca_size, batch_size):\r\n",
    "    dataset_cls = get_data_cls(task)\r\n",
    "\r\n",
    "    trainloader, pca, classes, words = get_data_loader(\r\n",
    "        dataset_cls, representations, pca_size,\r\n",
    "        'train', batch_size=batch_size, shuffle=True)\r\n",
    "    devloader, _, classes, words = get_data_loader(\r\n",
    "        dataset_cls, representations, pca_size,\r\n",
    "        'val', batch_size=batch_size, shuffle=False, pca=pca,\r\n",
    "        classes=classes, words=words)\r\n",
    "    return trainloader, devloader, devloader.dataset.n_classes, devloader.dataset.n_words\r\n",
    "\r\n",
    "trainloader, devloader, n_classes, n_words = get_data_loaders(\"semgraph_edge\", \"fast\", 600, 64)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "None\n",
      "None\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "from model import MLP, TransparentDataParallel\r\n",
    "\r\n",
    "def get_model(n_classes, n_words):\r\n",
    "    mlp = MLP(\r\n",
    "        \"semgraph_edge\", embedding_size=600, n_classes=n_classes, hidden_size=128,\r\n",
    "        nlayers=1, dropout=0.3, representation=\"fast\", n_words=n_words)\r\n",
    "\r\n",
    "    if torch.cuda.device_count() > 1:\r\n",
    "        mlp = TransparentDataParallel(mlp)\r\n",
    "    return mlp.to(device)\r\n",
    "  \r\n",
    "model = get_model(n_classes, n_words)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "import torch.nn as nn\r\n",
    "import torch.optim as optim\r\n",
    "from tqdm import tqdm\r\n",
    "from train_info import TrainInfo\r\n",
    "\r\n",
    "def train(trainloader, devloader, model, eval_batches, wait_iterations):\r\n",
    "    optimizer = optim.Adam(model.parameters())\r\n",
    "    criterion = nn.CrossEntropyLoss().to(device=device)\r\n",
    "\r\n",
    "    with tqdm(total=wait_iterations) as pbar:\r\n",
    "        mode_train_info = TrainInfo(pbar, wait_iterations, eval_batches)\r\n",
    "        while not mode_train_info.finish:\r\n",
    "            train_epoch(trainloader, devloader, model,\r\n",
    "                        optimizer, criterion, mode_train_info)\r\n",
    "\r\n",
    "    model.recover_best()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def _evaluate(evalloader, model):\r\n",
    "  #criterion = nn.CrossEntropyLoss().to(device=device)\r\n",
    "  dev_loss, dev_acc = 0, 0\r\n",
    "  for x, y in evalloader:\r\n",
    "    loss, acc = model.eval_batch(x, y)\r\n",
    "    dev_loss += loss\r\n",
    "    dev_acc += acc\r\n",
    "\r\n",
    "  n_instances = len(evalloader.dataset)\r\n",
    "  return {\r\n",
    "    'loss': dev_loss / n_instances,\r\n",
    "    'acc': dev_acc / n_instances\r\n",
    "  }\r\n",
    "\r\n",
    "def evaluate(evalloader, model):\r\n",
    "  model.eval()\r\n",
    "  with torch.no_grad():\r\n",
    "    result = _evaluate(evalloader, model)\r\n",
    "  model.train()\r\n",
    "  return result\r\n",
    "\r\n",
    "def train_epoch(trainloader, devloader, model, optimizer, criterion, mode_train_info):\r\n",
    "  for x, y in trainloader:\r\n",
    "    loss = model.train_batch(x, y, optimizer)\r\n",
    "    mode_train_info.new_batch(loss)\r\n",
    "\r\n",
    "    if mode_train_info.eval:\r\n",
    "      dev_results = evaluate(devloader, model)\r\n",
    "\r\n",
    "      if mode_train_info.is_best(dev_results):\r\n",
    "        model.set_best()\r\n",
    "      elif mode_train_info.finish:\r\n",
    "        mode_train_info.print_progress(dev_results)\r\n",
    "        return\r\n",
    "\r\n",
    "      mode_train_info.print_progress(dev_results)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def eval_all(model, trainloader, devloader):\r\n",
    "  train_results = evaluate(trainloader, model)\r\n",
    "  dev_results = evaluate(devloader, model)\r\n",
    "\r\n",
    "  print('Final loss. Train: %.4f Dev: %.4f', (train_results['loss'], dev_results['loss']))\r\n",
    "  print('Final acc. Train: %.4f Dev: %.4f', (train_results['acc'], dev_results['acc']))\r\n",
    "  return train_results, dev_results\r\n",
    "\r\n",
    "\r\n",
    "def save_results(model, train_results, dev_results, results_fname):\r\n",
    "  results = {'n_classes': model.n_classes,\r\n",
    "             'embedding_size': model.embedding_size,\r\n",
    "             'hidden_size': model.hidden_size,\r\n",
    "             'nlayers': model.nlayers,\r\n",
    "             'dropout_p': model.dropout_p,\r\n",
    "             'train_loss': train_results['loss'],\r\n",
    "             'dev_loss': dev_results['loss'],\r\n",
    "             'train_acc': train_results['acc'],\r\n",
    "             'dev_acc': dev_results['acc'],\r\n",
    "            }\r\n",
    "  with open(results_fname, \"w\") as write_file:\r\n",
    "    json.dump(results, write_file, indent=4)\r\n",
    "\r\n",
    "def save_checkpoints(model, train_results, dev_results):\r\n",
    "  model.save(\"checkpoints\")\r\n",
    "  results_fname = \"checkpoints\" + '/results.json'\r\n",
    "  save_results(model, train_results, dev_results, results_fname)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<>:5: SyntaxWarning: 'str' object is not callable; perhaps you missed a comma?\n",
      "<>:6: SyntaxWarning: 'str' object is not callable; perhaps you missed a comma?\n",
      "<>:5: SyntaxWarning: 'str' object is not callable; perhaps you missed a comma?\n",
      "<>:6: SyntaxWarning: 'str' object is not callable; perhaps you missed a comma?\n",
      "<ipython-input-9-7f01ee56427a>:5: SyntaxWarning: 'str' object is not callable; perhaps you missed a comma?\n",
      "  print('Final loss. Train: %.4f Dev: %.4f' (train_results['loss'], dev_results['loss']))\n",
      "<ipython-input-9-7f01ee56427a>:6: SyntaxWarning: 'str' object is not callable; perhaps you missed a comma?\n",
      "  print('Final acc. Train: %.4f Dev: %.4f' (train_results['acc'], dev_results['acc']))\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "train(trainloader, devloader, model, 100, 2000)\r\n",
    "train_results, dev_results = eval_all(model, trainloader, devloader)\r\n",
    "save_checkpoints(model, train_results, dev_results)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training loss: 1.0296 Dev loss: 1.6740 acc: 0.5630:  71%|███████▏  | 4573/6400 [02:19<00:59, 30.47it/s]"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}